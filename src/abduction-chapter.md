
---
bibliography: "/Users/lok/Dropbox/diss/repo/src/diss.bib"
csl: "/Users/lok/Dropbox/diss/repo/src/chicago-fullnote-bibliography.csl"
geometry: "left=1.5in,right=1in,top=1.5in,bottom=1.5in"
header-includes:
    - \setlength\parindent{24pt}
    - \usepackage{setspace}
    - \doublespacing
fontsize: 12pt
numbersections: true
title: The Strategic Role of the Weight of Evidence in Abduction
author: Lok C. Chan
---


<!-- 
The concept of evidential weight provides the framework Keynes needed to address Moore's concerns without falling back to the typical Bernoullian position. Keynes' response to Moore is that the severe uncertainty about the future does not prevent us from speaking of their probabilities - in Keynes word, they are "genuine" probabilities even if our knowledge about the remote future is slight. However, what distinguishes the probability of an immediate or near future event from one of event in the remote future is the degree of completeness of information involved. In other words, probability about the near future tends to be weightier evidentially than events in the remote future.

Many of the issues below hinge on different ways in which our ignorance about a state of affair could be characterized, so in order to avoid some confusion, I will define some terms here. I shall use "uncertainty" to refer to the epistemic attitude toward any event that involves some degree of ignorance and/or randomness. In other words, it is used as the broadest umbrella term that covers a cluster of similar epistemic attitudes; so, for instance, ambiguity and risk (as defined below) are different kinds of uncertainty. My propose usage of "risk" will follow Keynes' - the "risk" of an event is the definite and precise quantity, i.e., a probability, that captures our degree of belief toward it. I shall use "imprecision" to describe an event to which we cannot assign a _precise_ probability, but we can say _something_ about it, either in terms of an interval or by comparison. Keynes also think that there is also a kind of uncertainty about which we cannot say anything _at all_. This is what Frank Knight calls "uncertainty" in _Risk, Uncertainty, and Profit_, in which he defines "uncertainty" as being immeasurable uncertainty. For our purpose, we can call this "ambiguity". 




He is resistant to utilitarians' claim that the rational course of action can be wholly determined by calculating the precise "mathematical expectation", that is, what we called _expected utility_. [@keynes, ch. XXVI sec. 5-6]

This is where the concept of evidential weight is important, and where he disagrees with the expected utility theorists. To begin, the price of



It plays an important role in his ideas about economics and probability; thus,  One major reason for his skepticism is that expected utility does not reflect the amount of evidence involved in its calculation, which, as we shall see, piques Keynes' interest in the very notion of the weight of evidence. The other major reason is that Keynes holds that some probability relations cannot be determined precisely, and sometimes we can only give it an interval estimate, and sometimes it is completely indeterminate.





For the sake of clarity, we must first have a brief discussion on terminology.

As we shall see, in _The General Theory_, Keynes has adopted the distinction between uncertainty and risk to capture his thoughts about the weight of evidence. While Keynes was clearly aware of this distinction in _A Treatise_, this terminology seems to be more closely associated with  The distin

For the sake of clarity, I shall use the term "ambiguity" to refer to


In contrast to Knight, I try to avoid making out the distinction to be one about measurable vs unmeasurable, since I find that distinction itself unclear. For instance, Knight suggests that whether Britain would go to war unmeasurable, but I prefer to conceive the situation as being measurable, just with an extremely high degree of imprecision, since we can still say that the probability of Britain going to war is somewhere between 0 and 1.

These definitions are chosen simply because I find they they most easily reconciled with the relevant literature and what I find to be the most natural.

general theory

gamble device

secondary lit

refernce frame

Ellsberg


 -->

<!-- # Moore's Argument Against Expected Utility in the *Principia Ethica*

In chapter 1, we saw that Keynes' epistemology was squarely in the Moore-Russell tradition. Keynes' notion of probability as  intuitable logical relation is influenced by Moore moral intuitionism, a position that takes 'good' to be an indefinable but intuitable property of an object.[@mooreethics, p.9-10] Moore is, in particular, well known for his criticism of utilitarianism for committing what he calls the "naturalistic fallacy"[@mooreethics,p.11-12] The idea is that since 'good' is a basic unanalyzable property, it would be a mistake to, as utilitarians do, explicate it as another natural property such a pleasure or happiness. 

This, of course, flies in the face of any mathematical analysis of the right action, which requires a quantitative analysis of the amount of good generated by an action. More important, such an approach often requires the use of probability, since an action that produces some good with a low probability ought not be the same as one that generates the same degree of good but with a much higher probability. This is expressed clearly by Pascal and his fellow Jansenists in the *Port Royal Logic* in the 17th century: 

...in order to decide what we ought to do to obtain some good or avoid some harm, it is necessary to consider not only the good or harm in itself, but also the probability that it will or will not occur, and to view geometrically the proportion all these things have when taken together.[@artofthinking, p.273]

Moore's naturalistic fallacy can be seen as an attack on the use of the mathematical analysis in ethics, but he also has an argument against the use of *probability* in ethical reasoning. This deeply concerns Keynes. Moore's argument is offered in the chapter "Ethics in Relation to Conduct." His argument is that when we analyze the the good or utility generated by an action, we often only take into account the probability of the _immediate_ utility that would follow from the action, but to take utilitarianism seriously, Moore claims, we must consider the effects the action in the long the run.[@mooreethics, p.158] We have to know, for instance, whatever good our action produces in the immediate future will not be negated by the negative effects it has in the long run. However, Moore argues that, because so much about the future is currently unknown to us, we simply lack the ground to make good inferences about what effects each single action will probably case in the long run. For a somewhat crude example, consider a trolley-type problem in which I have to choose between saving a doctor or a criminal. I may reason that saving the doctor will more probably produce more utility in the future, since she will likely save more lives, but this only reflects the short-term outcomes. Why shouldn't I consider, for instance, the probability of the doctor engaging in malpractice and causing a significant amount of suffering, or the probability that the criminal might turn her life around and become a productive member of the society? 

Keynes resists this line of argument. To begin, Keynes takes Moore's argument to have demonstrated at most that we cannot know the long term consequence of our action with certainty, but this sets the bar too high---the whole point of employing probabilistic reasoning in our thinking about the right conduct is to maintain a degree of rationality despite of our ignorance about the future. A probability that is based on very little evidence is still "a genuine probability".[@keynes, 354]

Moore's conclusion, as Keynes points out, does make sense from a Frequentist point of view. Most, if not all, of our actions cannot be understood as a part of a long term frequency, so naturally the use of probability to determine the right conduct has a very little meaning from this perspective---if probability is strictly defined as something that expresses itself only in long term behavior, we cannot speak of the probability of an event with any credibility unless we can study the event in a controlled and repeatable environment.

Keynes' logical interpretation, however, does not require the meaning of a probability to be grounded in an empirically verified frequency. A probability is meaningful as long as we rightly perceive the logical relation between the premises and the conclusion. If we have no reason to think that our action is more likely to cause one long term consequence than another, then, on Keynes' interpretation of probability, we should rightly regard these events with indifference.[@keynes, p.354] 

While Keynes does not accept the conclusion of Moore's argument, he is deeply concerned by the incomplete nature of our knowledge of the remote future. To begin, we saw in chapter 1 that the application of Principle of Indifference has very strict conditions, and leads to contradictions when they are not followed, so unless these conditions obtain, we cannot properly calculate the expected value of our action. Keynes is also skeptical that all utilities can be precisely measured.[@keynes, p.356] 

Keynes also concerns himself with the relationship between evidence and expected utility, and this brings us back to the issue of the weight of evidence. -->
# Introduction

In Progress

# Peirce on Balance and Weight 

While the idea of evidential weight is closely associated with Keynesâ€™ discussion in *A Treatise*, Peirce was the first to bring our attention to its importance.[@probabilityofinduction] Like Keynes, Peirce was keenly aware of the distinction between the balance and the weight of the evidence. While Peirce develops the ideas on behalf of the Bayesian of his time in order to demonstrate its shortcoming, Peirce brings about formal develop of Bayesian ideas that come to be influential.^[Technically they were followers of Laplace, who developed many Bayesian ideas.] His development of the notion of the balance of the evidence is especially important. We already discussed Keynes' view on the notion, which is identified as the magnitude of a probability. Peirceâ€™s idea is a bit more involved---this is partly due to his concern with the viability of a based epistemology based on partial belief, while Keynes simply assumes a Russellian epistemology that serves as the philosophical springboard for his interpretation of probability.

Peirce takes that the balance of the evidence as an *epistemological* thesis about the justification of belief that has its root in Humean epistemology.[@essentialpeirce2 76] In *An Enquiry Concerning Human Understanding*, Hume makes his famous argument against the existence of miracle by appealing to the concept of probability. Hume argues that due to the fact that the only evidence we have regarding miracle is witness testimony, whether or not miracle exists can only be determined probabilistically by determining the balance of the evidence: 

>All probability, then, supposes an opposition of experiments and observations, where the one side is found to overbalance the other, and to produce a degree of evidence, proportioned to the superiority[@enquiry 97]

The idea is that probability should allow us to combine evidence for both sides of the argument, and determine which is favored by all the evidence we have. In addition, Hume thought that the balance of the evidence is not just a comparative measure, but also a quantitative one:

>In all cases, we must balance the opposite experiments, where they are opposite, and deduct the smaller number from the greater, in order to know the exact force of the superior evidence.[@enquiry 98]

Peirce thought that the best case for Bayesianism is to combine Hume's epistemology with the probability theory developed by Laplace. Peirce proposes a measurement of the belief $H$ on evidence $E$, we should take the odds of the probability of $H$ conditional on $E$ versus its probability conditional on $\neg E$. In other words,$$\frac{P(H|E)}{P(\neg H | E)}$$ Which can be algebraically decomposed into $$\frac{P(E|H)P(H)}{P(E|\neg H) P(\neg H)}$$
Using Principle of Indifference, which will be discussed below, the above becomes $\frac{H}{\neg H} = 1$, the balance of evidence is simply $\frac{P(E|H)}{P(E| \neg H)}$, which is an expression of what we now call the Bayes Factor. Peirce then proposes that Hume's idea can be captured by taking the log of the ratio. Consider the following expression: $$\frac{P(E|H)P(H)}{P(E|\neg H) P(\neg H)} = log(P(E|H)) - log(P(E|\neg H))$$ A positive value means the evidence is in favor of H and negative against it. It also captures Hume's idea that the strengths of two independent pieces of evidence should be combinable. Suppose $E_1$ and $E_2$ are independent. So, $P(E_1 \wedge E_2 |H) = P(E_1|H) P(E_2|H)$. If we take the logarithm of the product, the product becomes $log(P(E_1|H))+ log(P(E_2|H))$.  This captures the idea that adding two pieces of independent evidence together should increase the intensity of our belief. Some modern Bayesians were so impressed with Peirce's account that they incorporated Peirce's ideas into their own frameworks. I. J. Good refers to Peirce as a precursor of his account.[@goodpeirce] Branden Fitelson cites Peirce as the inspiration of his  Bayesian account of independent  evidence.[@Fitelson2001-FITABA] 

However, Peirce argues that the balance of the evidence alone cannot be satisfactory capture our state of belief. Peirce illustrates the idea roughly as follows:  imagine two urns $A$ and $B$ with unknown proportions of black and white balls. Suppose you sample (with replacement) 100 balls from the urn $A$ and find 50 black balls and 50 white balls. Justifiably, you infer that the proportion of black balls in $A$ - call it $\theta_A$ is about $0.5$. You then decide to sample from $B$, but this time you only manage to draw 4 samples, 2 of which are black balls. Your best estimate for $\theta_B$ is $0.5$. At this point, I offer you another chance to draw from one of the urns, and if you manage to draw a black ball from that urn, you get $100. Which urn would you pick?

Clearly, $\theta_B = \theta_A$, but it is unclear if we should regard them with indifference, for a simple reason: the evidence for $\theta_A = 0.5$ is more substantial than the evidence for $\theta_B = 0.5$. 

What Peirce has in mind, it should be clear, is precisely what Keynes later calls the weight of evidence. However, while Keynes was hesitant in affirming the necessity of evidential weight, Peirce thinks its role is indispensable:

> to express the proper state of our belief, not one number but two are requisite, the first depending on the inferred probability, the second on the amount of knowledge on which that probability is based.[@probabilityofinduction, 296]

Unfortunately, however, this is the only instance of Peirce's explicit discussion of the weight of evidence. It would seem that the matter would not be broached again until Keynes' discussion in his *Treatise*, which comes to entirely overshadow Peirce's discussion of it. Even now, as Peirce scholars come to be interested in Peirce's discussion, it tends to be approached as a historical question of whether or how Perice has anticipated the later conceptions of evidential weight, such as Keynes' and Good's. [@Kasser2016-KASTCO-6] However, what I will attempt is to construct an account of the weight of evidence within a largely Peircean framework. This will show that many puzzles regarding the weight of evidence that arise from Keynes' understanding can be resolved with Peirce's epistemology of inquiry.

<!--  Peirce points out, however, this can happen in many ways. First of all, for a typical urn problem, if we have sampled many times from the urn, our belief will be resilient and will unlikely to be changed by new evidence. But this can happen even if 
 -->

# Abduction 
<!-- bring in peirce here? Peirce's idea: urgency-->
[in progress]

An important key to a Peircean interpretation of the weight of evidence is Peirce's tripartite classification of reasoning. As is well known, Peirce is responsible for coining the phrase "abduction", which is distinguished from deduction and induction. Peirce's understanding deduction and induction is very close to the modern understanding of the distinction. Deduction considers the necessary consequences between propositions, so a derivation in, say, predicate logic and theorem proving would be considered as deduction by Peirce. Induction, on the other hand, involves inference that is neither necessary nor justified by the relation between propositions, but by its reliability and conduciveness to knowledge. In scientific context, deduction is supposed to tease out the logical consequences of a hypothesis, which are then treated as predictions to be tested using inductive methods.

Where does abduction come in then? abduction is "the first starting of a hypothesis and the entertaining of it."[@CP, 6.525] This must be understood in the context of the context of Peirce's epistemology: inquiry---the practice of generating knowledge---is an interplay between doubt and belief. Doubt and belief are distinguished by their psychological and practical differences: psychologically, doubt is an uneasy and unstable state, while belief is characterized by imperturbability---a sense of stability of the mind.[@fixation, 247] Practically, a belief expresses itself as as a stable habit---an established second nature, while the instability of doubt prompts us to seek satisfaction through reaching a state of belief. The degree of a belief, in turn, expresses itself in terms of the decisiveness of action.^[This is not to say Peirce believes in subjective probability. While Peirce has indicated that beliefs can be measured quantitatively, as far as the interpretation of probability goes, he is an objective propensity theorist.]

<!-- A quick sketch of this picture reveals that abduction, at least for Peirce, is *not* inference to the best explanation, even though the two are often confused.[@howdidabduction] Roughly speaking, inference to the best explanation is the thesis that a hypothesis can be justified by how well it *explains* the evidence we have. Abduction, on the other hand, is not even always about the explanatory power of the hypothesis, even though it can often be. While Peirce's view on abduction evolves throughout his life, he is consistent in holding that abduction is the preparatory step the inquirer takes before the experiment or observation. For instance, in an early paper which still refer to abduction as "hypothesis," Peirce says that the abduced hypothesis 'should be distinctly put as a question, before making the observations which are to test its truth.'[@deinhyp, 331]  -->


Thus, Aduction is a reaction to the irradiation of doubt, caused by the perception of some surprising facts. [@CP, 5.512] It is an inferential process in which a hypothesis is proposed as a possible account for the the surprising fact. 

Thus, the role of abduction is paramount to the growth of knowledge, as it is "the only logical operation which introduces new ideas".[@essentialpeirce2 216] For an analogous position, consider Thomas Kuhn's idea that a shift in scientific paradigm has to be initiated by the perception of some anomalous event: Roentgen's the discovery of X-rays, instance, was caused by his perception of the surprising fact that his barium platinocyanide screen was unexpectedly glowing. [@kuhn 57-58] 

Peirce often uses the following syllogistic schema to illustrate the inferential process of abduction:

1. The surprising fact, C, is observed; 
2. But if A were true, C would be a matter of course. 
3. Hence, there is reason to suspect that A is true.[@CP, 5.189]

<!-- absuction is important -->
In this schema, $C$ is the observed *fact* that calls for a hypothesis that would, in Peirce's words, *rationalize* it. [@essentialpeirce2 107] Peirce sometimes refers to $A$ as an *abductive suggestion* that follows the perception of the surprising fact.[@essentialpeirce2 227] They are *possible* accounts for the phenomenon observed, and are not presented as true proposition, but hypotheses that we may accept provisionally for the sake of further testing. Thus, in abduction we *infer* that a particular hypothesis is worthy of investigation. 


<!-- abduction is just guessing -->
How, then, does abductive reasoning guide us in coming up with productive hypothesis that accounts for the facts observed? If abduction is to be a species of reasoning, on par with deduction and induction, there has to be some principled way to distinguish good abductions from the bad ones. Yet Peirce's characterization of abduction sometimes the process almost seems either magical or random:

>The abductive suggest comes to us like a flash. It is an act of insight, although of extremely infallible insight. [@essentialpeirce2 227]

>...abduction is, after all, nothing but guessing.[@CP 7.219]

<!-- This suggests that the rationality of abduction cannot be captured as a set of rules. Yet, Peirce insists 

that abduction, although it is very little hampered by logical rules, nevertheless is logical inference, asserting its conclusion only problematically or conjecturally, it is true, but nevertheless having a perfectly definite logical form. [@essentialpeirce2 231] -->

However, just because abduction is ultimately guessing does not mean that it involves nothing but luck. An important idea is that the beginning of abduction usually involves a leap of faith, but this leap can be made *strategically*.  Peirce explains the strategic nature of abduction with an analogy of the game of "twenty questions", in which one party has to think of an object, while another party has to find out what the object by asking 20 or less questions.[@essentialpeirce 109] Peirce's idea is that posing a question in the game is akin to proposing a hypothesis in an inquiry---for both cases, we make a guess about the nature of the subject of our inquiry, and the difference is that in the game the answer we receive is certain and direct, while a hypothesis has to be tested or broken down into sub-hypothesis. Peirce has an important point in mind, however, with this example. That is, the choice of a question can drastically alter the course of an inquiry. Since the question of the game has to figure out what the answer is by asking at most 20 questions, she ask them strategically. Each question the inquire asks should be as informative as possible---they should, for instance, ask questions that narrow the space of possibilities as quickly as possible. Abduction, which is the logic of hypothesis selection, is guided by a similar goal.

Hintikka suggests that looking at the role played by strategic thinking in deductive thinking is helpful in understanding what Peirce has in mind. Hintikka points out that even though there is an emphasis on mechanical rules of inference, deductive thinking involves more than that. Suppose a student is asked, in an exam for a logic class, to demonstrate that $$\vdash [[A\to(B\wedge \neg C)] \wedge (\neg B \vee D) ]\to (A\to D)$$ In other words, to prove that it is a logical truth. Before proving that, she must make a decision on which method to use. Since she is pressed for time, she must choose to prove this strategically. For instance, suppose she choose the strategy of trying to show that it is always true. one way would be to use the truth table to show that the proposition is true in all possible interpretations. But that would be a inefficient choice, since there will be $2^4=16$ rows and she cannot skip any row since she's trying to prove a logical truth. Alternatively, she could prove it using natural deduction, which calls more further strategic thinking: an often recommended strategy for proving logical truth is by *Reductio ad Absurdum*, that is, by showing a contradiction from the negation of the said logical truth. But in this case, this would hardly seem like a good decision, since she would end up with a long negated conditional. A better strategy would be to prove the result conditionally assuming the antecedent of the conditional. Depending on the rules available to her, such a method would take 10 or less lines. So, a successful deductive reasoner must know all the basic rules of inferences, but she must also master policies or strategies that are more suggestive in nature. 


The important point here is that the strategic dimension of reasoning often finding a way to gain as much information as possible in light of some constraint. In the game of twenty questions, the constraint is that the questioner only gets 20 questions. For the student taking a logic exam, she is bound by the time she has. In both cases, the reasoner has to make a decision on confronting the problem she faces in an efficient manner. In particular, before even tackling the problem they are interested, both reasoners must select one of many methods in which the problem could be addressed. Often, the efficacy of the chosen method cannot be determined until it has been actually tried, so there will be always be an element of guessing involved. 

The strategic dimension of abduction, as Peirce sees it, pertains to how our resources is allocated in service to our epistemological goal, and this is addressed by a branch of abductive reasoning called *the economy of research*. The creative and suggestive nature of abductive inference makes it permissive in nature, and, for it to be reliable, it must be guided by economical considerations that discriminate between productive and unproductive proposals. According to the schema Peirce provides, we can infer that there is a reason to suspect a hypothesis to be true, as long as it accounts for the facts somehow. To be sure, Peirce makes it clear that the permissive nature of abduction must be tempered by the pragmatic maxim---a hypothesis must be distinguishable by their experimental and practical consequences.[@essentialpeirce2 234] Still, for many facts, there could be countless possible testable accounts, and examining every single possible abductive suggestion is not only imprudent, but it impedes greatly our ability to find the right hypothesis.[@CP 2.776] Thus, how to deal with this problem, in light of our limited resources, is the heart of the economy of research and the rationality of abduction:


> Proposals for hypotheses inundate us in an overwhelming flood, while the process of verification to which each one must be subjected before it can count as at all an item, even of likely knowledge, is so very costly in time, energy, and money---and consequently in ideas which might have been had for that time, energy, and money, that Economy would override every other consideration even if there were any other serious considerations. In fact there are no others.[@CP 5.602]

<!-- 
Another helpful comparison, due to Hintikka, is to consider an inferential move in abduction as being *strategy-based* instead of being rule-based.


 has made the proposal that Peirce's abduction should be understood as a kind of *strategy* based reasoning, instead of a *rule* based one.
 -->

Thus, even though 



The key here is to see abduction both as a *creative* and *rational* procedure. On one hand, 

Explanatory virtue is obviously one guide---the proposed hypothesis must explain the fact in some way



Hintikka's suggestion 



<!-- not justified -->

strategic

My claim is that the weight of evidence is relevant not in the inductive stage of inquiry, but in the abductive stage. To begin, 


More important, abduction

First, the weight of evidence pertains to the *strategic* stage of abduction

Example: Recall one of Richard Jeffrey's responses to Popper's paradox of ideal evidence is that the prior we choose affects how we learn. 

illustration: generate random sample

Second, we saw that Skyrms's idea of resiliency is the measure of a probability's relative stability against a certain surprising  

# economy of resarch
For Peirce,, the weight of evidence is tightly connected to the notion of the *precision* of a probability, as he suggests it should be based on what he calls probable error, which is a precursor to the modern Frequentist notion of confidence interval.[@tamingofchance] This is evidenced by Peirce's example above---it anticipated what is now known as the *Ellsburg Paradox*, an empirical phenomenon in which human subjects are shown to be sensitive to difference between precise(a point value) and imprecise(an interval) probabilities.[@ellsberg] Peirce was impressed by Boole's way of deriving indefinite probabilities by using arbitrary constants.[@peirceboole, 239] Boole's view is seen as the precursor of modern theory of imprecise probability.[@walley 43] It is then no surprise that Peirce is cognizant of the nuance issues surrounding the precision of probability.

Peirce, furthermore, has a dynamic view on the weight of evidence. Unlike Keynes, Peirce does not seem to think the weight of evidence *always* increase when new relevant evidence is introduced; because, he thinks that the weight of evidence is tied to the probability's "liability to be changed by further experience".[@probabilityofinduction, 295] So---to use the terminology from chapter 1---when our belief is resilient, the weight of the evidence will not change even if new evidence is introduced.

# Moore's Anti-Utilitarian Argument and The Problem of Total Evidence

Peirce's insight is closely related to a puzzle between evidential weight and expected utility in *A Treatise in Probability.* In chapter 1, we saw that Keynes' epistemology was squarely in the Moore-Russell tradition. Keynes' notion of probability as  intuitable logical relation is influenced by Moore moral intuitionism, a position that takes 'good' to be an indefinable but intuitable property of an object.[@mooreethics, p.9-10] Moore is, in particular, well known for his criticism of utilitarianism for committing what he calls the "naturalistic fallacy"[@mooreethics,p.11-12] The idea is that since 'good' is a basic unanalyzable property, it would be a mistake to, as utilitarians do, explicate it as another natural property such a pleasure or happiness. 

This, of course, flies in the face of any mathematical analysis of the right action, which requires a quantitative analysis of the amount of good generated by an action. More important, such an approach often requires the use of probability, since an action that produces some good with a low probability ought not be the same as one that generates the same degree of good but with a much higher probability. This is expressed clearly by Pascal and his fellow Jansenists in the *Port Royal Logic* in the 17th century: 

> ...in order to decide what we ought to do to obtain some good or avoid some harm, it is necessary to consider not only the good or harm in itself, but also the probability that it will or will not occur, and to view geometrically the proportion all these things have when taken together.[@artofthinking, p.273]

Moore's naturalistic fallacy can be seen as an attack on the use of the mathematical analysis in ethics, but he also has an argument against the use of *probability* in ethical reasoning. This deeply concerns Keynes. Moore's argument is offered in the chapter "Ethics in Relation to Conduct." His argument is that when we analyze the the good or utility generated by an action, we often only take into account the probability of the _immediate_ utility that would follow from the action, but to take utilitarianism seriously, Moore claims, we must consider the effects the action in the long the run.[@mooreethics, p.158] We have to know, for instance, whatever good our action produces in the immediate future will not be negated by the negative effects it has in the long run. However, Moore argues that, because so much about the future is currently unknown to us, we simply lack the ground to make good inferences about what effects each single action will probably case in the long run. For a somewhat crude example, consider a trolley-type problem in which I have to choose between saving a doctor or a criminal. I may reason that saving the doctor will more probably produce more utility in the future, since she will likely save more lives, but this only reflects the short-term outcomes. Why shouldn't I consider, for instance, the probability of the doctor engaging in malpractice and causing a significant amount of suffering, or the probability that the criminal might turn her life around and become a productive member of the society? 


## Challenge 1: 

Keynes resists this line of argument. To begin, Keynes takes Moore's argument to have demonstrated at most that we cannot know the long term consequence of our action with certainty, but this sets the bar too high---the whole point of employing probabilistic reasoning in our thinking about the right conduct is to maintain a degree of rationality despite of our ignorance about the future. A probability that is based on very little evidence is still "a genuine probability".[@keynes, 354]

Moore's conclusion, as Keynes points out, does make sense from a Frequentist point of view. Most, if not all, of our actions cannot be understood as a part of a long term frequency, so naturally the use of probability to determine the right conduct has a very little meaning from this perspective---if probability is strictly defined as something that expresses itself only in long term behavior, we cannot speak of the probability of an event with any credibility unless we can study the event in a controlled and repeatable environment.

Keynes' logical interpretation, however, does not require the meaning of a probability to be grounded in an empirically verified frequency. A probability is meaningful as long as we rightly perceive the logical relation between the premises and the conclusion. If we have no reason to think that our action is more likely to cause one long term consequence than another, then, on Keynes' interpretation of probability, we should rightly regard these events with indifference.[@keynes, p.354] 
<!-- 
While Keynes does not accept the conclusion of Moore's argument, he is deeply concerned by the incomplete nature of our knowledge of the remote future. To begin, we saw in chapter 1 that the application of Principle of Indifference has very strict conditions, and leads to contradictions when they are not followed, so unless these conditions obtain, we cannot properly calculate the expected value of our action. Keynes is also skeptical that all utilities can be precisely measured.[@keynes, p.356]  -->

## Challenge 2 WOE

Keynes, however, is unwilling to wholeheartedly endorse the doctrine of the maximization of utility---he is especially concerned with the fact that expected utility does not account for the weight of evidence, since utility is only discounted by the magnitude of its probability. So as far as expected utility theorists are concerned, if the two probabilities have the same value, it makes no practical difference if the probability is based on the Principle of Indifference, or a big dataset that is uniformly distributed.  

However, Keynes points out that proponents of expected utility theory often *does* implicitly assume the importance of the weight of evidence. Bernoulli, for instance, suggests that rationality demands the utilization of all evidence available to us. This implies, Keynes thinks, that it's always rational to get more evidence, but then it raises another critical question about whether or not one could ever be rational in refusing new evidence. [@keynes, p.84-85] If the answer for the former question is positive, and the latter question negative, then we have to conclude that rationality dictates us that we should never stop looking for more evidence.

Keynes does not make the jump from "using all the evidence" to "get all the evidence" clear. Nevertheless, this problem is revisited many years later in an exchange between Ayer and Carnap. So drawing a parallel between them could be helpful. 

In his *Logical Foundation of Probability*, Carnap restates Bernoulli's maxim as "the requirement of total evidence".

> _Requirement of total evidence_: in the application of inductive logic to a given knowledge situation, the total evidence available must be taken as basis for determining the degree of confirmation.[@carnapprob, p.211]

Aver, in response to Carnap, raises the Keynesian question: should "total evidence" include relevant evidence that I do not yet have in possession?[@ayerpae, p.56] The answer must be "yes", Ayer argues. If finding the truth value of some proposition $P$ could potentially sway the balance of my evidence, then I should definitely acquire it. Thus the principle of total evidence seems to suggest that I am also rationally compelled to consider some evidence I do not yet have.

But Ayer points out that this cannot be the whole picture: taken as a rule of rationality, this means we should never stop acquiring unless we are certain that we have acquired all available evidence. This, however, assumes that we know what evidence is available, but it could often be unrealistic to expect to know how much evidence we *do not* currently have. 

Ayer argues that this is only a symptom of a deeper problem about logical nature of probability and its bearing on the rationality of our evidential practice. The logical interpretation of probability, held by both Carnap and Keynes, takes probability as a logical relation between propositions, so within this picture, inductive rationality is a matter of having the right degrees of belief between premises and conclusion---this is analogous to the idea deductive rationality is perceive correctly whether the conclusion follows necessarily from the premises. 

Logic, however, does not care about how much evidence we have; it only cares about the relation between our propositions. This problem affects the subjective interpretation of probability as well. As Leonard Savage points out, probability could reveal to us the incoherence within the web of our belief, but it cannot tell us how to resolve it.

> According to the personalistic view, the role of the mathematical theory of probability is to enable the person using it to detect inconsistencies in his own real or envisaged behavior. It is also understood that, having detected an inconsistency, he will remove it. An inconsistency is typically removable in many different ways, among which the theory gives no guidance for choosing.[@savage, p.57]

We encountered a version of this problem in chapter 1, in which we considered Keynes' definition of the weight of evidence in terms of the conditional relevance. Once again, the weight of evidence seems directly relevant to inductive reasoning, yet it cannot be easily situated in the probabilistic framework. The notion of resiliency, which we discussed in detail in chapter 1, does not seem to do any better. While it captures the expression of the weight of evidence, it does very little in illuminating on how it dedicates the rationality of our decisions. Both Joyce and Skyrms are silent on this.

<!-- Our original problem of how the severe uncertainty affects the rationality of our current action has seemingly decomposed into two different problems: the original problem about the uncertainty of the remote future and a problem regarding the rationality of our evidential practice. I content that these two problems are in fact both sides of the same coin: the uncertain nature of the remote future is not something we can change in a substantial way---the real question is how we can rationalize our current action by reasonably projecting stability into the future.  -->

<!-- This property of evidential weight was already apparent in our analysis of the concept of resiliency. In terms of Skyrms' conditional resilience, we saw that the more evidence we have, the more resilient a belief tends to get.
 -->
<!-- It is rather unfortunate that Keynes has not further elaborated on this. The idea that the demand of evidence scales with the amount we have, in addition to the problem with the strict definition discussed in chapter 1, should make it relatively clear to Keynes that the weight of evidence cannot increase whenever relevant evidence has been introduced. On the other hand, the relativized notion of weight, implicit from his other remarks, dovetails nicely with the concepts of resilience discussed in chapter 1. 

-->

<!-- What seems to be the lesson here is that the weight of evidence should be a measure of the *sufficiency* of the evidence, by informing us if the evidence we have at hand is *enough*.

In chapter 1, we saw that evidential weight can manifest as a resilience of belief. 
 -->

<!-- Note, however, that this question can take two forms: the first is a question about finding a threshold of sufficiency: we increase the weight of our evidence until it reaches this threshold, and the second is to *define* sufficiency *in terms* of weight: our body of evidence is sufficient, when relevant evidence no longer increases its weight. The conceptual connection between the sufficiency and weight is tighter in the second form than the first, and this is the route Ramsey takes in his attempt to answer Keynes' question. -->

<!-- rephrase this so it's more like an illustration of keynes -->

A crucial background on my use of these terms is Peirce's pragmatism, which is often identified with his *pragmatic maxim*, a slogan that summarizes the overarching approach of the pragmatist. Peirce fine-tunes his definition of pragmatism throughout his career,  but the most well-known version is contained in an early paper, "How to Make our Ideas Clear":

> Consider what effects, that might conceivably have practical bearings, we conceive the object of our conception to have. Then, our conception of these effects is the whole of our conception of the object. [@makeideasclear, p.266]

Peirce was explicitly dissatisfied with this expression. Part of the problem is that it is often interpreted as expressing a crude verificationism, which says something along the lines of words owe their meanings entirely to verifiable sense experience.  This is how Ayer, for instance, reads Peirce. Ayer takes Peirce's view to be essentially verificationism, except, instead defining a word by its verifiable experience, it reduces the meaning of a word to "listing the facts to which the word applies" [@ayer-origin, 51]. Peirce's impatience with unproductive disputes is, without a doubt, an attitude with which a logical positivist like Ayer sympathizes, but to say "that Peirce's pragmatic maxim is indeed identical, for all practical purposes, with the physicalist interpretation of the verification principle" has clearly missed the mark. [@ayer-origin, 55] Eager to conscript Peirce into his own camp, Ayer attributes to him a very strong skepticism about theoretical entities. This reading, however, misreads the pragmatic maxim: it does not say a concept is reduced to its actual effects; instead, it states that concepts are delineated by the effects "that might *conceivably* have practical bearing, we *conceive* the object of conception to have". 

What, then, does Peirce have in mind? What Ayer misses, I think, is that Peirce is concerned with how our beliefs and conceptions about the world have bearing on the rationality of our intellectual conducts. Peirce toward the end of his life puts emphasis on this point in his characterization of pragmatism:

> [Pragmatism is] the maxim that the entire meaning and significance of any conception lies in its conceivably practical bearings, - not certainly altogether in consequences that would influence our conduct so far as we can force our future circumstances but which in conceivable circumstances would go to determine how *we should deliberately act*, and how we should act in a practical way and not merely how we should act as affirming or denying the conception to be cleared up. [@essentialpeirce2, 145, my emphasis]

In 1905, Peirce, to "eliminate some unsuspected source of perplexity," restates the pragmatic maxim as follows:

> The entire intellectual purport of any symbol consists in the total of all general modes of rational conduct, which conditionally upon all possible different circumstances and desires, ensue up the acceptance of the symbol.[@essentialpeirce2, 346]

One striking difference between these formulations and the one from "How to Make Our Ideas Clear" is emphasis on deliberative action and rational conduct. "Deliberate conduct," Peirce further explains, is "self-controlled conduct." [@essentialpeirce2, 348] The crucial idea here is that since the meaning of a word manifests itself through its causal and practical effects, a belief, which consists of the use of these words, should have a practical effect on those who accepts the belief. But the effect here is not one of a causal one, but a rational one. Thus, Peirce has a very conception of rationality very similar to van Fraassen's: rationality is not about providing the pedigree of our current opinions, but the rational constraint on they have on the agent's future conduct, for "future facts are the only facts we can, in a measure, control"[@essentialpeirce2, 359]. 

# The Weight and Value of Evidence


Keynes, again, is rather ambivalent about resolving the tension between the weight of evidence and how inductive reasoning is generally conceived. In pass, however, he makes a tantalizing suggestion that the *utility* of evidence could be the component needed to resolve the situation. He begins with the observation that getting evidence for a belief low in evidential weight will "probably produce the greatest amount of good", but the situation is opposite when the evidence for the belief is weighty---"there clearly comes a point when it is no longer worth while to spend trouble"[@keynes, 84-5] Thus, for a hypothesis of interest $H$, the same evidence $E$ generates different amount of utility relative to the amount of information we already have for $H$. If an agent has almost no information about $H$, gathering more information would generate the most utility, but for the same evidence, the demand might to low, because the agent might already have enough information about $H$, so getting more evidence would yield very little to no utility. 

This opens the door to another conception of the weight of evidence, one that is not purely based in the *amount* of evidence, but also on how much how much we *want*. In the *Treatise*, Keynes does switch implicitly these two way of thinking of evidential weight---sometimes refers the weight of evidence a balance "between the absolute amounts of relevant knowledge and of relevant ignorance respectively"[@keynes, p. 78] In a later chapter of the *Treatise*, he also calls weight "the degree of completeness of the information"[@keynes, p.357] These remarks suggest that weight is about how much evidence we desire as much as how much we *do* know. This also allows the possibility that new relevant evidence does not always increase the weight of the evidence, because if I already have enough evidence, getting more might not always be informative. 


The puzzle about the utility of evidence, and its bearing on the rationality of the gathering of evidence, has been addressed by Ramsey on an unpublished note. Interestingly, however, he has proven essentially the opposite conclusion reached by Ayer and Keynes: Ramsey shows that we should always look more more evidence, because we can never be worse off from doing it. How can this be? 

Ramsey's argument is roughly that, _if_ we assume an agent to be a perfect Bayesian and that new information does not cost anything, then she will never be no worse off getting new evidence.[@ramseyvalue, also see @goodtotalevidence and @savage, sec 6.2] In fact, she is guaranteed to be *better* off as long as the new evidence will tell her something new. A perfect Bayesian agent is someone who studiously updates her opinions based on Bayes' rule and then act by choosing the action that maximize her expected utility. Note that this assumes two things: first, for any decision problem she faces, there is always going at least one course of action that maximizes her expected utility, and second, as Skyrms points out, this also implies that the agent knows that she will always *stays* being perfectly Bayesian in the future.

I will make use of an intuitive example rather than reproducing the proof here.^[This example is adapted from @leviweight] Suppose we have three hypotheses about the content of an urn in front of us:

1.  $H_b$: 90 black balls and 10 white balls
2.  $H_w$: 10 white balls and 90 black balls
3.  $H_n$: 50 white balls and 50 black balls.

Suppose we start by assuming $P(H_b) = P(H_w) = P(H_n) = 1/3$---we could have some knowledge that assures us that these are the only three possibilities. There is also a reward of \$1 for picking the correct hypothesis. Our expected payoff for choosing each hypothesis would be the same at $1/3$. Nevertheless, we are allowed to sample with replacement as many times as we wish. Should we get more evidence? Yes, according to Ramsey, we should, and this can demonstrated in terms of an expected utility analysis.

To begin, at this point, the probability of getting a black ball is the same as getting a white ball. Let $E_b$ be "a black ball is drawn" and $E_w$ for white balls. So:
$$P(E_b) = P(H_b)P(E_b|H_b) + P(H_w)P(E_b|H_w) + P(H_n)P(E_b|H_n)$$
$$=1/3(0.9)+1/3(0.1)+1/3(0.5)=0.5$$
And $P(E_w) = 1 - P(E_b) = 0.5$. So, in the event of drawing a black ball from the urn, we would update our belief like so:

$$P(H_b|E_b) = \frac{P(H_b)P(E_b|H_b)}{P(E_b)}=\frac{1/3(0.9)}{0.5} = 0.6 $$

Similarly, applying the calculation on the other hypotheses, we get:
$$P(H_w|E_b) = 0.067$$
$$P(H_n|E_b) = 0.333$$
Similar argument can be made by assuming $E_w$, that is, a white ball is chosen. In that case $P(H_w|E_w) = 0.6$. If we were an ideal Bayesian agent, we should pick $H_b$ if $E_b$, and pick $H_w$ if $E_w$. Since an ideal Bayesian would choose the option that maximizes our expected utility, in either case the expected value after drawing from the urn once is $0.6$, which is an improvement, since before drawing our expected utility is $1/3$ for all options. The net gain in expected utility would be $0.6 - 0.33 = 0.27$, is referred to as *the value of information* in the decision theory literature.[@appliedstatdec p.89-90. For a more digestible presentation see @winkler sec.6.3]

It turns out that we would be even better off if we were to draw from the urn again. Suppose the first draw yields a black ball. So now we have one piece of evidence in hand. Let us refer to our state of belief after the first draw as $H_b', E_b',..$ and so on. For instance, $P(H_b') = P(H_b|E_b)$ and $P(E_b') = P(E_b'|E_b)$. One notable change is that $P(E_b') = 0.7132$ and $P(E_w')=0.2868$. If we draw again and get a black ball, this means:

$$P(H_b'|E_b') = 0.757$$
$$P(H_w'|E_b') = 0.009$$
$$P(H_n'|E_b') = 0.233$$

If a white ball were to be drawn:

$$P(H_b'|E_w') = 0.21$$
$$P(H_w'|E_w') = 0.21$$
$$P(H_n'|E_w') = 0.58$$

Thus, if the second sample is a black ball, we would choose $b$ since it has the maximum expected utility at $0.757$, and if we get a white ball, we choose $n$ with the expected value at $0.58$. So, the expected utility, if we were to draw from the urn again, is: $0.7132(0.757) + 0.2867(0.58) = 0.706$, which is an improvement over just drawing once. The net gain is $0.706 - 0.6 = 0.106$. Ramsey's proof shows that we can keep on getting more evidence and we will never be worse off. In fact, we will be better off as long as there is evidence out there we do not yet have.

What should we make of Ramsey's proof? There are two issues involved here. The first is Keynes' observation that evidence can have a diminishing return, so relevant evidence does not increase weight, and the other is how the weight of evidence bears on the rationality of our action, especially when it comes to the gathering evidence. Ramsey's note provides a good answer for the former, but not the latter. 

<!-- Ramsey's note is tantalizing, because he never clearly explains what he thinks the weight of evidence is, even though it's in the title. But assuming that he is interpreting Keynes' question of the weight of evidence as whether in relation to the value or worth of evidence, it is not far fetched to think that he is thinking evidential weight in terms for the expected utility that new evidence will generate.  Ramsey might have something like the following in mind: the weight of new evidence $E$ for hypothesis $H$ is the difference between the prior expected utility $EU(H)$ and the posterior $EU(H|E)$. Let's call this Ramseyian weight. -->

To begin, Ramsey's contribution here is a way for us to think about the relationship between the *weight* of the evidence in possession and the *value* of the potential new evidence.^[As noted, this is essentially the idea of the value of information in decision theory, but, as a historical note, Ramsey, inspired by Keynes' puzzle about the weight of evidence, has anticipated this development by many years.] Ramsey clearly thought is the value of evidence $E$ for hypothesis $H$ as something along the line of the difference between the prior expected utility $EU(H)$ and the posterior $EU(H|E)$.

For instance, we saw that in the example above, the posterior expected utility of the first draw was $0.27$ higher than our prior expected utility, and we saw a net gain of $0.106$ in expected utility if we were to draw again after drawing a black ball, so we see that the first piece of evidence has a higher value than the second one. What Ramsey's proof demonstrates is that new evidence has a diminishing return---I get a "bigger bang for the buck" for my evidence gathering endeavor when I have less evidence. In light of this, Keynes' example of the balance of the evidence unchanged by the introduction of new evidence is then somewhat incomplete. This explains one of Keynes' puzzle about worthiness of our endeavor to get more evidence in light of the evidence we have in possession.

However, the broader normative question is still unanswered: how should the weight of evidence guide the rationality of our action? To be sure, I do not question that given some assumptions, Ramsey's result will necessarily follow: the same result is proven by both Good and Leonard Savage, so there is no doubt that the result will holds if the assumptions are granted, but that's a big _if_---we have to question if how often these assumptions actually hold.

<!-- Ramsey probably understood that information was rarely free. However, Ramsey might have interpreted Keynes' puzzle not as a *decision problem about evidence* but a question regarding its intrinsic value. We saw that Ayer essentially posed the same question to Carnap. History essentially repeated itself when I.J. Good puts forth essentially the answer to Ayer. Interestingly, Good interprets Ayer's as questioning "why... we should bother to make new observations." [@goodthinking, p.178] So, Good seem to think what is needed is a justification for getting new evidence _in general_. Ramsey might have interpret Keynes in the same way. With respect to this version of the problem, the proof makes perfect sense, since it demonstrates that all things being equal we usually end up with better expected utility by considering more evidence. But Keynes' question was about reconciling the general duty to get more evidence and the intuition that evidence gathering for a belief of interest is not always a worthy endeavor.  -->


The scenario we imagined quickly breaks down once we starts to introduce some sort of cost. It was assumed in the example that it costs us neither money nor time to draw from the urn, but suppose it costs us 25 cents for each sample. This means that we would be gaining only $0.27-0.25 = 0.02$ in expected payoff for the first draw, and the second draw would definitely not be worth the additional 25 cents. Or suppose that one dollar is not worthy any endeavor that lasts longer than 15 seconds, and it takes 30 seconds to draw from the urn. As soon as minimally realistic assumptions are introduced, Ramsey's result no longer holds.

Cost might also enter into consideration in different forms, e.g., computational cost or memory. In the same context, Savage ponders over an interesting case that introduces yet another dimension of the problem: consider a very ill person, who is given the option to find out with no cost if the disease she has is mortal. Savage points out that an argument can be made that in this case refusing information could be rational. The thought is that the patient may decide that, based on an assessment of her own personality, she would live the rest of her remaining life in agony if she were to find out that her disease is very serious, whereas she could live relatively happily without knowing. Savage's response is that in this case the information is not really free; it has a *psychological* cost.[@savage, p.107] 

Savage's response was intended to paint a counterexample as an explainable exception to the idea that it's always rational to get free evidence, but this sort of implicit cost is the norm, not an exception: at the minimal, any endeavor to seek more evidence will cost us at least time, and the loss of time is the loss of opportunity. Cost might also enter into consideration in different forms, e.g., computational cost or memory. In light of this, Ramsey's demonstration---the value of evidence depends on how much evidence we have and how much evidence there is to get---does not ease Keynes' concern at all, but it makes the question of evidence gathering more pressing: I might not be getting the most out of my evidential endeavor if it turns out that I could be much better off by examining a different hypothesis, for the very set up for Ramsey's proof requires us to have already chosen *one* hypothesis of interest. But in reality, we often have competing interests. The question is not whether I will get some utility out of gathering evidence for hypotheses in which I have interest, but how I should go about doing it, given the limitation of resource.

 Even though van Fraassen has come to downplay this strategy, it is still illustrative. The trick to this particular Dutchbook argument is a subjective probability conditional on a higher order probability---a probability about one's future belief. Since the structure of this argument is perfectly analogous to the one discussed above, let us skip ahead and talk about the bet. Suppose the Duke basketball team is playing against UNC tonight at 8pm. The time is currently 1pm. Your friend asks you for your subjective probability 4 hours later that you will be willing to bet on Duke winning at odds 2:1. For the sake of clarity, let us define these propositions:

\begin{centering}

$D$: Duke will win at 8pm. 

$B_{t}$: at time $t$pm, P(D) = 1/3.

\end{centering}

Upon reflection, I respond that 

$$P(B_5) = 0.4$$

Now suppose my friend asks me to reflect on my higher order reasoning: what is the probability that my predicted estimate will be wrong? More specifically, what is the probability that $(\neg D \wedge B_{5})$? Suppose I respond that 

$$P(\neg D \wedge B_{5} ) = 0.3$$

From this, $P(\neg D|B_5)$ is derivable: 

\begin{align}
P(\neg D|B_5) &= \frac{P(\neg D \wedge B_5)}{P(B_5)}= \frac{0.3}{0.4}=0.75
\end{align}

And $P(D|B_5) = 1 - 0.75 = 0.25$. Now, with this information, my friend then stages a Dutchbook situation against me with the following bets:

|  Bet |Condition   |Reward   | Cost  |
|---|---|---|--:|
| 1  | $(\neg D \wedge B_5)$  | 1  | $(1)P(\neg D \wedge B_5) =0.3$ |
|  2 |   $\neg B_5$| 0.75  |  $(0.75)P(B_5)=0.45$ |
|  3 | $B_5$ | 0.083 | $(0.083)P(B_5) = 0.03$  |

The trick is that, in order to devise a Dutchbook against me, the reward for bet 2 has to be $P(\neg D | B_5)$, for bet 3 it has to be $P(\neg D | B_5)$ minus my subjective probability of $P(\neg D)$ at 5pm, which is $0.75-2/3= 0.083$. Since the costs for these bets were calculated using expected utility, I should regard all of them as being fair. All three bets cost me $0.78$ in total. Now, at 5pm, there are two possible outcomes:

1. I do not come to believe that $P(D) = 1/3$: I win bet 2, but lost 1 and 3. This leads to a net loss of $-0.78 + 0.75 = -0.03$.
2. I come to believe that $P(D) = 1/3$. I get $0.083$ for winning bet 3. Now bet 1 is now contingent on whether or not $\neg D$. My friend now offers me $2/3$ to buy back that bet, which is fair in my light. I sell that bet, which renders the result of the game irrelevant. In this case, I have a net loss of $-0.70+0.083+2/3 = -0.03$. 

Again, we arX. We do not have put all our faith in a Dutchbook argument, but it is illustrate to see how one could get out of it, and that is by following what van Fraassen calls the principle of Reflection:

\begin{center}
(Refection):$P_t(A|p_{t+x} = r)=r$
\end{center}

This says: your probability of $A$ at time $t$, given you also believe that at some later point in time $t+x$ your subjective probability for $A$ will be $r$, should also be $r$. It will be helpful to see how in the Duke basketball case I have failed to follow this principle. Recall that the current $t$ is 1, so $B_5$ is essentially the same as $p_{1+4}(D) = 1/3$---four hours later, I will come to believe that $P(D)=1/3$. Also recall that we have derived that $P(\neg | B_5) = 0.25$. This means that I have violated the Reflection, for



$$P_1(D|p_{5} = 1/3)=0.25 \neq 1/3$$
But how would following Reflection h. As before, suppose that my probability of $p_5{D} = 1/3$ is 0.4. When my friend asks for my subjective probability for $P(\neg D \wedge (p_5(D)=1/3))$, I should have used Reflection to determine that $P_1(D|p_{5} = 1/3)=1/3$. So
\begin{align*}
P_1(\neg D \wedge (p_5(D)=1/3))) &= P_1(\neg D |p_5(D)=1/3) \times P_1(p_5(D)=1/3)\\
&= (1-1/3) \times 0.4\\
&= 0.27
\end{align*}

So this means that my pre-Reflection respond $0.3$ was $0.03$ too high, and this discrepancy is exactly how much I was sure to lose due to being Dutchbooked. 

|  Bet |Condition   |Reward   | Cost  | 
|---|---|---|--:|
| 1  | $(\neg D \wedge (p_{5} = 1/3))$  | 1  | $(1)P(\neg D \wedge B_5) =0.27$ |
|  2 |   $\neg (p_{5} = 1/3)$| 0.675  |  $(0.75)P(B_5)=0.27$ |
|  3 | $(p_{5} = 1/3)$ | 0.008 | $(0.008)P(B_5) = 0.003$  |



ccording to this interpretation, making a probabilistic judgment is what philosophers of language call *a speech act*.[@beliefwill 254] Van Fraassen uses the act of making a promise---a classic case of speech case---as a parallel. Suppose I borrow some money from a friend, and I say to her "I promise that I will pay you back by next week." I am not merely reporting my feeling of sincerity, even though that might well be an accurate description. I am also making a commitment that opens myself to criticism by assuming the obligation to repay my friend: if I fail to discharge this promise, my character might be criticized or questioned. In terms more familiar to probabilists, making a promise involve putting oneself at the *risk* of losing credibility or having my integrity put into question.

The expression of a probabilistic judgment, to be sure, is quite different than making the 

1. I promise that I will repay you, but I do not believe that I will.
2. It's raining, but I do not believe that it's raining.



Van Fraassen shows that by arguing for what he calls the principle of Reflection. We will discuss the principle formally in chapter 2, so I will present it informally here. There are two formulations of Reflect, special and general. The General Reflection Principle says that my current opinion about some event $A$ should belong to the set of possibility opinions held by my conception of my future self, otherwise one is irrational.[@beliefuly 16]

Roughly speaking, the principle says that our degree of belief for some belief now must be with the range of what we envision our future self will find possible. For instance, if I believe that I will come to have a subjective probability of belief $X$ 

Hence voluntarism "give[s] central importance to the will and the role of decision", because the view does suggest that [@empiricalstance 77], 
<!-- commitment -->

<!-- reflection -->

A cornerstone for voluntarism is the principle of Reflection. One way to motivate this principle is through the Dutchbook argument.


# Epistemic Agency

<!-- 
To further discuss these issues, we shall now turn to Peirce, who concerned himself with what he calls "the economy of research" through out his intellectual life.

 -->

<!-- To advance my position, however, it will be illuminating to see *why* Ramsey's assumptions do not hold, which provides the context for why the Peircean solution I offer is needed.  -->
<!-- What we really have to consider are three different outcomes after the introduction of relevant evidence: -->

<!-- 1. Both balance and weight are changed by new evidence.
2. Balance remains the same, but the weight is changed by new evidence.
3. Neither balance nor weight is changed by new evidence. -->
<!-- 
We saw that outcome 3 was not possible under the Keynes' definition  examined in the first section, since according to that definition the introduction of evidence *always* increases its weight, but Ramsey's expected utility approach can account this, since the expected utility that new evidence brings us will approach zero as we exhaust all available evidence. This way of thinking about evidential weight explains how weight cannot be measured just in terms of the amount of relevant evidence at hand: if we are almost always better off getting more evidence, we should incorporate and acquire as much new evidence as possible, but since evidence has a diminishing return, at a certain that new evidence will no longer raise our expected utility in a meaningful way (even though it will also never decrease it). This means that at that point the new evidence will no longer have any Ramseyian weight, since the posterior expected utility will stay the same, even though it would have been weighty if we have no prior evidence.  -->



<!-- 
Of course, Ramsey probably understood that information was rarely free. It is clear that he intended his note to be an response to Keynes' remarks on weight of evidence, as the title of his note is "Weight or the Value of Knowledge," which suggests that he interprets the problem Keynes poses as a question why evidence is valuable. Good interprets Ayer's remarks in the same way. According to Good, Ayer is questioning "why... we should bother to make new observations." [@goodthinking, p.178] So, both Ramsey and Good seem to think what is needed is a justification for getting new evidence _in general_. With respect to this version of the problem, the proof makes perfect sense, since it demonstrates that all things being equal we usually end up with better expected utility by considering more evidence. But Keynes' question was about reconciling the general duty to get more evidence and the intuition that evidence gathering for a belief of interest is not always a worthy endeavor.

Good, who proved the same result independently of Ramsey, tries to address this issue by distinguishing what he calls Type I and Type II rationality.^[@goodthinking p. 29-30 As far as I could tell, this has nothing to do with the distinction between Type I and Type II error in Frequentist statistics.] Type I rationality is that of the ideal Bayesian agent, one who lives her life by abiding to the principle of maximizing expected utility. Good recognizes, however, that type I rationality provides no guidance in regard to when an investigation should be concluded. This is where type II rationality comes in: it is principle of maximizing expected utility _plus_ the consideration of "the cost of theorizing." More important, the goal of type II is "a sufficient maturity of judgments."[@goodthinking, p.29]

<!-- rephrase this so it's more like my idea -->
<!-- Good's distinction suggests that there are aspects of evidential practice that cannot be captured by the inductive ideal expressed by Type I rationality. In other words, "a sufficient maturity of judgments" cannot just be a matter of maximizing our expected utility. Intuitively, maturity implies a sort of *stability*---a sort of imperturbability against confounding experience. We can interpret Keynes' puzzle in this light: perhaps sometimes it is rational to refuse evidence, because my belief has reached a degree of maturity such that the same evidence that would have been highly relevant at beginning is now uninformative. -->

#
<!-- 

My contention is that the normative role of the weight of evidence is to guide hypothesis selection in abductive reasoning. 

Abductive reasoning is guided by what Peirce calls the *economy of research*, which involves the analysis of the trade off between the utility of new knowledge and its cost.

The weight of evidence figures into the economy of research as a conceptual component of what Peirce calls *economic urgency*.

Peirce often uses the game "20 questions" to illustrate 

Hinttika's idea that abduction is a type of strategic inference is helpful here.


This explains 



The thesis that the utility we get from evidence scales with the weight of our evidence is at the heart of Peirce's "Note on the Theory of the Economy of Research", written many years before Ramsey's attempt on the problem. Peirce suggests the crucial problem regarding the relations between evidence and utility is "how with a given expenditure of money, time, and energy, to obtain the most valuable addition to our knowledge"[@econofre, 72]. Thus, as far as Peirce is concerned, the pressing epistemological problem is not to ask whether it is rational to get evidence, but how to get them rationally on the limited resource available to us. I think that this is very much in line with how Keynes has in mind---the problem of the joint satisfaction of the requirement of total evidence and the maximization of expected utility cannot be resolved without considering how cost affects the evidential endeavors. If cost is no object by assumption, the problem is so trivialized that it can hardly be called a problem. -->
<!-- 
One definitive feature of Peirce pragmatist philosophy is that any intellectual concept must be clarified and differentiated by their *practical consequences*; otherwise, it should be considered as unfit for the use in any epistemic or scientific endeavor. The weight of evidence should not be different.
 -->

<!-- Peirce seems to think that the increase of the weight of evidence often, though not always, results in the increase in precision.



However, Peirce takes the evidential weight of a probability judgment to be more conceptually connected 

More important, Peirce thinks that the connection between the weight of evidence and precision of probability provides a way in which the utility of evidence can be quantified---the very notion of epistemic progress can be explicated as the decrease in imprecision. [@econofre 72] -->
<!-- 
hidden strcuture = uncertainty

# Abduction and The Choice of Hypothesis


The weight of evidence is the same. The sort of situation that Peirce has in mind, in which the weight of evidence makes a practical difference, is not unlike Popper's example of a coin with an unknown bias and a coin you know to be absolutely fair. As we have seen in the last chapter, we see that there is a meaningful difference between the two even from a Bayesian statistical point of view, but Peirce tends to think that academic examples like those downplays the problem the weight of evidence represents. In a typical urn example, the proportion of urn is often already known. In such an example, Peirce points out, there is very little reason to bother with the weight of evidence, and he even uses the language of resiliency to express this idea: when the content of the urn are already known, "the number which expresses the uncertainty of the assumed probability and its liability to be changed by further experience may become insignificant, or utterly vanish."[@probabilityofinduction, 295]

But real inquiry is almost never like that---even the example we considered earlier, in which we try to determine one of 
 -->
<!-- To begin, Peirce believes that evidential weight is indispensable in the representation of our beliefs -->

<!-- This is because Peirce is more committed in the practical role the weight of evidence has---*because* evidential weight *does* make a difference in action, it must be accounted for some how. --> 
<!-- >when our knowledge is very precise... the number which expresses the uncertainty of the assumed probability and its liability to be changed by further experience may become insignificant, or utterly vanish. But, when our knowledge is very slight, this number may be even more important than the probability itself; and when we have no knowledge at all this completely overwhelms the other [@probabilityofinduction, 295-296]

 -->

<!-- Long before Keynes, Peirce was keenly aware of the role the weight of evidence plays in the economy of research. To begin, the notion of the weight of evidence plays a central role in Peirce's critique of conceptualism, a subjective view on probability held by followers of Bayes and Laplace, characterized by the acceptance of the Principle of Indifference, and the doctrine that all degrees of belief can be assigned a precise value. Anticipating Keynes' distinction, Peirce argues that conceptualism fails to recognize that



Even thoguh Peirce never gives a name to this second number that encapsulates "the amount of knowledge on which that probability is based", it is clear that he has something like Keynes' notion of the weight of evidence in mind. More important, he is much more acutely in tune to the dynamic between the weight of evidence and the state of our knowledge. He points out that 
 -->
<!-- urgency -->


<!-- 

Peirce was keenly aware of the cost an investigator would encounter over the course of the inquiry: the very act of hypothesizing would cost us 
abduction is 

To Peirce, that is a much more difficult problem to solve than a proof for the intrinsic value of evidence, as he puts it: -->

<!-- >Proposals for hypotheses inundate us in an overwhelming flood, while the process of verification to which each one must be subjected before it can count as at all an item, even of likely knowledge, is so very costly in time, energy, and money---and consequently in ideas which might have been had for that time, energy, and money, that Economy would override every other consideration even if there were any other serious considerations. In fact there are no others.[@CP 5.602] -->
<!-- 
Thus, Peirce is keenly aware of how the growth of knowledge is a costly endeavor---even the very act of theorizing  -->

<!-- 
# Conclusion

In this chapter I have argued for the position that the weight of evidence plays a *strategic* role in abductive reasoning. In the *Treatise*, Keynes sees the importance of the weight of evidence in light of Moore's anti-utilitarian argument against making probability judgment due to long term uncertainty. I attempted to use Peirce's thoughts on abductive reasoning to explain this interplay between the weight of evidence  -->

# Reference