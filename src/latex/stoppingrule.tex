
\hypertarget{deliberation-and-the-problem-of-optional-stopping}{%
\chapter{Deliberation and the Problem of Optional
Stopping}\label{ch:stopping}}

In the last chapter, I tried to incorporate the Peircean notion of
abduction into a broadly probabilist framework through an inferentialist-pragmatist reading of van
Fraasseen's Reflection Principle and voluntarism about belief. I sketched out a framework that aims to accommodate the
voluntarist idea that epistemic judgments are speech acts with normative
implications, and Peirce's conception of rationality as deliberate
conduct. Peirce's rich notion of abduction, which
goes beyond the mere appeal to explanatory values, supplies the context-sensitivity needed to understand the voluntarist commitments codified by the Reflection Principle.

The main lesson, as I suggested, is that the normative force that regulates
epistemic commitments one undertakes in making a probabilistic judgment cannot
be understood without taking into account the \emph{context} of inquiry in which the judgment is made. An
assertion, as Peirce suggests, has no normative force unless the
assertion is underwritten by an epistemic practice that incentivizes the
agent to stand by the obligations imposed upon her. Dutch book
scenarios, though in general unrealistic, can be seen as one such
context, since in the setup the agent is stipulated to make bets and
revise her degrees of belief in a specific way.

More needs to be said about these contexts, especially how they work within experimental practices of hypothesis testing. In this chapter, I endeavor to develop my position in the specific
context of inductive and statistical inference. To summarize the position, I 
defend the following slogan:
\paragraph{The Deliberativist Thesis} Inductive inference must be interpreted in
light of the agent's commitments, intentions, and values that frame the deliberative framework in which the deliberate data gathering procedure is structured.\\


What I call a \emph{deliberative framework} is the context of induction chosen in the
abductive stage: it is the provisional selection of the hypothesis to be provisionally
accepted and probed, the probabilistic judgments deduced from the
statistical model used to represent the phenomenon of interest, and
experimental procedure to practically engage in these assumptions. The
term ``deliberative'' is designed capture Peirce's idea, as discussed in \ref{sec:inferentialism-and-the-context-of-deliberate-conduct}, that accepting a
belief or making a judgment means one is rationally required to conduct oneself in
a deliberate way, in accordance with the relevant inferential commitments. Even a provisionally accepted hypothesis implies the agent's commitment to test the viability of it. Many elements in this deliberative framework, moreover, cannot be determined mechanically, and must be decided upon according to on the experimenter's intentions and values.

I will defend and develop the deliberativist thesis by critically
examining the \emph{problem of optional stopping}, one of the many points of
contention in the Bayesian-frequentist debate. The gist of this problem
is that the frequentist conception of statistical evidence is dependent
on the intentions of the experimenters, such that the experimenter may
manipulate the evidence just by changing her intention to stop the
sampling. This, Bayesians argue, is detrimental to the validity of
frequentist inference. Instead, many Bayesians hold that the \emph{total} evidential
import of data can be fully captured by what is called the
\emph{likelihood function}, which is impervious to the effects of the
agent's intentions. This is called \emph{the Likelihood Principle}.

My goal of this purpose is to resist the Likelihood Principle by
addressing the problem of optional stopping. Since denying the involvement of intentions in statistical inference implies the rejection of the deliberative thesis, I do not propose to show that the effects of optional stopping do not exist; instead, I argue that deliberative elements such as the intention to stop are relevant also to Bayesian reasoning, so the deliberativist thesis holds within the
Bayesian context.

In section 3.1, I will give a historical presentation of the problem of
optional stopping in the context of its used by
parapsychologists to, essentially, cook up evidence for the existence of
``extra-sensory perception'' (ESP). In section 3.2, I will sharpen
Bayesians' argument that the statistical involvement of agent-intentions is to be blamed for optional stopping, and why
they think this is a critical flaw of frequentism. In section 3.3,
however, I will explain how the same problem can apply to Bayesan
reasoning. In the last section, 3.4, I examine a potential response to
my argument by considering a proof about \emph{the value of evidence}
given by Ramsey and Savage.

\hypertarget{esp-and-optional-stopping}{%
\section{ESP and Optional Stopping}\label{sec:esp-and-optional-stopping}}

On April 24th, 1940, the mathematician W. Feller delivered a lecture on
his critique of the statistical method used in parapsychological
research at a Duke mathematic seminar.\footnote{\cite{felleresp}, 281} At that time, J. B. Rhine was
spearheading Duke's parapsychology research: to make parapsychology
scientifically respectable, Rhine believed, statistical evidence must be
used to support the conclusions he hoped to demonstrate.\footnote{\cite{elusive}, 108} Feller points
out, however, many results of these experiments involve on a trick
called ``optional stopping'', which is used to abuse statistics to get
their desired outcome. Feller argued that such experimental practices
invalidate the result of parapsychological studies. Feller's specific
criticism against parapsychology, however, became the starting pointing
of a general critique of frequentist statistical methods, often
mobilized by Bayesian statisticians. The argument is that, while the
parapsychologists no doubt had questionable experimental practices, it is
a flaw of the frequentist methods they employed. The problem, Bayesians
argue, is that a statistical conclusion ought not be influenced by
extra-statistical concerns such as when the experimenter decides to
stop.

One of the phenomena for which parapsychologists claimed to have found
statistical evidence is \emph{extrasensory perception} (ESP), i.e., that some
people can perceive certain facts without the use of any of the five
senses. How can such a claim be examined experimentally and
statistically?

\begin{figure*}[h] 
\begin{center}
\includegraphics[scale=0.4]{zener.png}
	\caption{\href{https://commons.wikimedia.org/wiki/File:Zener_cards_(color).svg}{Zener Cards}, by Mikhail Ryazanov. Licensed under \href{https://creativecommons.org/licenses/by-sa/3.0/deed.en}{CC BY-SA 3.0}}
	\label{fig:zen}
\end{center}	
\end{figure*}



One often used experimental setup for testing ESP is an activity called
``card guessing,'' using the so-called Zener cards, after Karl Zener, the
Duke psychologist who suggested the design to Rhine.\footnote{\cite{rhineesp},48} A Zener card can have one of 5
unique symbols. A typical deck of Zener cards contain 5 cards for each
symbol. A trial of this experiment typically involves a subject
being repeatedly asked to guess the face of a randomly chosen card,
while the investigator would note the actual face of the card and the
subject's answer. After each trial, the subject's reported sequence would then be
compared to the observed result, and a score would be calculated based
on the number of correct guesses.

Of course, one could achieve a high score by chance: no one would think
I had ESP if I successfully had predicted the outcome of a coin flip,
because we know that, no matter what my prediction is, I would still
have a \(0.5\) probability of getting it right. But what if I guessed
correctly the result of 10 consecutive tosses? The probability of that
is \(0.5^{10} = 0.001\). We are much more inclined to say that I have
some sort parapsychological ability, because it would have been an extraordinary
coincidence if I got it right purely by chance. This is the sort of
statistical argument that Rhine and his followers tried to make. Their
contention is that if a subject can attain a score that is too
extraordinary to be explained by chance, then we have statistical
evidence for the person's ESP ability. In statistics, the probability of
an outcome, assuming that it is by chance, is called the $p$ value.

Since there are 5 faces, a subject has the probability of \(0.2\) of
getting it right just by guessing alone, so someone with ESP should do
better than that. This is how ``better'' can be explicated: suppose a trial with 100 attempts has been carried out on a
subject. If the subject is just guessing, then we would expect that she
would get around 20 cards right. In fact, according to the theory of probability, we can ascertain that out of 100 cards, there is a
probability of \(0.944\) that the subject can get 26 or fewer correct
guesses. Conversely, the probability that a guesser can get 27 or more
cards right is pretty low: \emph{if} that is what we observed, the p value is \(0.056\). The frequentist idea is that using probability, we can explain what ``better than chance'' is with much greater clarity, but it still requires an epistemic judgment: I may decide that witnessing something that only has a $0.056$ probability of occurrence is enough to imply a discrepancy between my provisionally chosen hypothesis and experience that I am willing to admit it is wrong. If I decide to follow standard statistical practice, I would need a value below $0.05$, in which case I would as a rule reject my initial hypothesis if the subject can get 27 hits, since the probability for that would be $0.03$. A parapsychologist who desires her field to be accepted by the community, I suspect, would want to choose a standard that is agreeable to the people she wants to convince---skeptical scientists.

Of course, this point between ``better than chance'' or not also entirely depends on how long the trial
is. In other words, we say that 27 is the cutoff, only because we
already decided that the trial involves 100 guesses. For 500 guesses,
for instance, there is a probability of 0.95 to randomly guess 115 cards
correctly. So, using the same standard, scores higher than 115 would be
considered as statistical evidence for the hypothesis the subject
actually has ESP.

\begin{table}[]
\centering
\begin{tabular}{l|c|c|c}
 Subject& Guesses &Hits  & Std. Dev. \\\hline
 M.C.& 1250 & 243 &  -0.49\\
H.F. &1000  & 219 & 1.50 \\
H.H. & 2000 & 416 & 0.90 \\
A.K. & 1000 & 212 & 1.50 \\
 H.S.& 1000 & 195 & -0.39 \\
 T.S. & 1000 & 222 & 1.73 \\
 J.T.& 1000 & 210 & 0.79 \\
 E.S.& 1750 & 370 & 1.19 \\
 A.M.& 500 & 118 & 2.01 \\
 L.S.& 500 & 113 & 1.45 \\
 D.A.& 750 & 168 &  1.64\\
 O.W.& 500 & 114 &  1.56
\end{tabular}
\caption{Card-guessing performance of manic-depressive patients (Shulman, 1938, 101)}
\label{psychotable}
\end{table}

One study claims to have discovered just that: a parapsychologist
carried out the card guessing experiment on 141 patients in a mental
hospital.\footnote{\cite{psyesp}} The study claims to have found statistical evidence that
manic-depressive individuals have demonstrated the ability to detect the
face of a card through extrasensory means. It is said that these
subjects consistently scored higher than chance. For instance, consider
patient A. M. who got 118 hits out of 500 attempts. As discussed, it
would seem that, assuming A. M. was just guessing, it would have been an
extraordinary coincidence that he scored 18 higher than normally
expected. In fact, the p value---the probability for an outcome like this or
better to happen by chance---is \(0.027\), which is quite
improbable. Does this constitute evidence for ESP?

Feller argues that this result is spurious, because the
parapsychologists practiced \emph{optional stopping}.\footnote{\cite{felleresp}, 291-292} The idea is that
many of these experiments have no set number of attempts, and often
an experiment could stop either exactly when a favorable result is
obtained, or a ``break'' would be taken if the subject is not doing as well as they expect. For instance, an experiment could be terminated early in order
preserve a significant result. A. M., for instance, has made 500
attempts. His test was then much shorter than his peers, many of who
made more than 1000 guesses. The question is, then had A.M. guessed more cards, would his performance regress toward what would expect from guessing? 

Of course, it impossible to ascertain this now. But to make this point more tangible, we can take advantage of the power of modern
statistical computing: we can simulate experiments with similar stopping
rules, and see if we can get $p$ values below $0.05$. The difference here is that for the simulation we \emph{know}
that the participants are just guessing, so we know that any statistical significance would be a fluke. That is, if we can still force
significance using optional stopping, then there is a good reason to
doubt the supposed evidence for ESP gathered using optional stopping. We need to stimulate the following scenario: the experimenter will randomly draw a Zener card out of a shuffled deck, with replacement.
She will then ask the subject to guess the card, and record the result.
For each subject, she will stop under one of these two conditions: \emph{either} the result has reached significance---the probability of the current outcome is less than \(0.05\)---\emph{or} 2000 guesses have been made. The
experimenter will then move on to the next subject, until she has
examined 1000 subjects. All subjects are guessing, so their probability
of success is exactly \(0.2\).

More precisely, the simulation consists of the following procedure: 
\begin{enumerate}
	\item The simulation will run 1000 times, each of which represents one subject. 
	\item For each subject $i$, a random sample will be drawn from a Bernoulli distribution, with the probability of success $p=0.2$, to simulate a subject who will randomly guess the face of a randomly drawn Zener card, with replacement. 
	\item For each draw $k$ (of subject $i$), the result $x_{i,j}\in \{0,1\}$ will be added to the total sum $y_i$ for the subject.
	\item Each trial continues to run, \emph{until} one of the following conditions is obtained on the $n$th guess:\begin{description}
		\item[significance:] When the $p$ value is less than or equal to $0.05$. That is, 
		$$P(Y\geq y_i) = \sum_{j=y_1}^n {n \choose j} 0.2^j 0.8^{n-j}\leq 0.05$$
		\item[end:] $n=2000$: the subject has made 2000 guesses without significance. In this case, we simply move on to the next subject.
	\end{description} 
\end{enumerate}

\begin{figure*}[h] 
\begin{center}
\includegraphics[scale=0.5]{fixedvsoptionalsuc.png}
	\caption{\emph{Left}: successes out of 1000 random samples drawn from $Binomial(n=2000,p=0.2)$. \emph{Right}: successes for all optional stopping simulations.}
	\label{fig:freqhistsuc}
\end{center}	
\end{figure*}


\begin{figure*}[h] 
\begin{center}
\includegraphics[scale=0.4]{freqstoppval.png}
	\caption{Histogram of p values($\leq 0.05$)}
	\label{fig:freqhistpval}
\end{center}	
\end{figure*}

\begin{figure*}[h] 
\begin{center}
\includegraphics[scale=0.5]{fixedvsoptional.png}
	\caption{\emph{Left}: p values for 1000 random samples drawn from $Binomial(n=2000,p=0.2)$. \emph{Right}: p values for all optional stopping simulations.}
	\label{fig:freqfixedvsoptional}
\end{center}	
\end{figure*}

\begin{figure*}[h] 
\begin{center}
\includegraphics[scale=0.4]{freqstopn.png}
	\caption{Histogram of the number of attempts among optionally stopped simulation.}
	\label{fig:freqhistn}
\end{center}	
\end{figure*}

Here's a summary of the result, after simulating a test of 1000 subjects:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Starting with \ref{fig:freqhistsuc}, it is helpful to graphically examine the difference in shapes between trials with a fixed $n=2000$  number of attempts and trials with optional stopping allowed. In accordance with the central limit theorem, the left hand side forms a nice normal distribution.\footnote{\cite{degroot}, section 6.3} The right hand side with optional stopping forms a bimodal distribution.
  
  \item As seen in \ref{fig:freqfixedvsoptional}, 371 out of 1000 outcomes has reached significance (\(\text{p value}\leq0.05\)). P values for trials with fixed N (left) are almost uniformly distributed.
\item
  A $p$ value as low as \(0.017\) was obtained. A distribution of significant p-values with optionally stopping allowed can be see in the right figure in\ref{fig:freqfixedvsoptional}. Simulations that allowed optional stopping clearly have a much higher chance to reach significance, even though we already know the true probability is $0.2$. 
\item
  On average, significant results stopped at the 294th attempt. The
  median is 85. According to figure  \ref{fig:freqhistn}, a substantial number of significant results were obtained by stopping before $400$ guesses. 
\item Successful tests tended to stop early. This makes intuitive sense, since we know that the larger the sample size is, the closer the result will approximate the true proportion. This point will be reexamined in section \ref{sec:intentions-and-self-sabotage-redux}, in the context of the argument from intention. There, we will see that the negative binomial distribution, which models trials with no fixed $n$, tend to encourage false positives when the sample size is small. \end{enumerate}



Thus, using optional stopping, we can easily find `evidence' for ESP if
we look long enough.

Even though Feller was primarily concerned with demonstrating the shoddy experimental
practices of parapsychologists, these problems would later be used by Bayesians as part of their critique of frequentism. The basic argument is that
parapsychologists could cheat in way they did, because of the way in
which the probabilities are computed and interpreted using frequentist methods, and these problems
are supposed to avoidable within the framework of Bayesian statistics.
In the next section, we will review this argument.

\hypertarget{likelihood-and-counterfactual-probabilities}{%
\section{Likelihood and Counterfactual
Probabilities}\label{likelihood-and-counterfactual-probabilities}}

From the problem of optional stopping then, what follows? From the
Bayesian perspective, the implication is twofold---a criticism of frequentism and a reason for using Bayesian statistics. The criticism says that the
experimenter's intention to stop could directly influence the
significance of the result is because of frequentists' reliance on
\emph{counterfactual probabilities}. This provides a reason for switching to Bayesian methods, since they, the argument goes, do not require counterfactual probabilities, and, therefore, are immune to the argument from intention. I propose to first examine these two lines of thought, and then transition to the epistemological issues.

Suppose two cards were randomly chosen from the deck, and a subject was able to guess the face of both cards. What is the probability that a subject who is guessing will manage  both hits? To begin, there are four ways in which an experiment with
two attempts could have turned out. Let \(H\) be ``Hit'' and \(M\) be
``Miss''. The four possibilities are: \[MM \quad MH \quad HM \quad HH\] But
to get the probabilities needed, we will have to decide on the probability of a Hit: if we accept \emph{provisionally} that a subject is just making randomly guesses, then the probability of a Hit is $0.2$, and the probability of a Miss is $0.8$. With these assumptions in place, we can calculate the so-called \emph{sampling distribution}.

\[P(H  H)=0.04 \quad P(H  M)=0.16  \]
\[P(M H)=0.2(0.8)=0.16 \quad P(M M)=0.8^2=0.64 \]

This is also represented in the first row of \ref{tab:likelihoodvscf}. 
The frequentist argument is that if the $p$ value---the probability of the data observed is low enough to be considered statistical significant. Now, Bayesians argue that the $p$ values cannot be
seen as an unadulterated summaries of the observed evidence, since any $p$ value is laden with assumptions about what might have happened, based on hypothesized parameters. Consider table \ref{tab:likelihoodvscf}. What frequentists are recommending here is that, when we evaluate a hypothesis, we should consider the probability of the observed data by comparing the relative frequency of its occurrence in comparison to the other values on  the same row, which are all calculated based on the assumption that the ``random guessing'' hypothesis is true. Therefore, when we say that $HH$ is an extraordinary event, we are really saying that it is extraordinary \emph{relative to the three other scenarios that we could see, but did not.}

\begin{table}[]
\centering
\begin{tabular}{@{}c||c|rrr@{}}
\toprule
 & Factual & \multicolumn{3}{l}{} Counterfactual\\ \midrule
Hypothesized Reliability & HH & HM      & MH      &  MM    \\\midrule
$\theta=0.2$ & 0.04 &   0.16    &   0.16    &    0.64  \\
 $\theta=0.5$& 0.25 &    0.25   &   0.25    &    0.25  \\
 $\theta=1.0$& 1.00 &   0.00    &   0.00    &   0.00   \\ \bottomrule
\end{tabular}
  \label{tab:likelihoodvscf}
  \caption{Comparison between sampling distributions (rows) and likelihood functions (columns)}
\end{table}

Bayesians, such as Lindley, criticize the use of probabilities of counterfactual scenarios in the evaluation of statistical hypotheses:


\begin{quote}
The usual statistical significance test requires the sample space, or alternatively, the stopping rule to be specified. Many people's intuition says this specification is irrelevant... Of what relevance are things that might have happened, but did not?\footnote{\cite{lindleybern}, 114.}
\end{quote}

Jaynes shares this sentiment:

\begin{quote}
The question of how often a given situation would arise is utterly
irrelevant to the question how we should reason when it does arise. I
don't know how many times this simple fact will have to be pointed out
before statisticians of ``frequentist'' persuasions will take note of
it.\footnote{\cite{jaynesmight},247}
\end{quote}

How, then, should we look at evidence? The Bayesian perspective focuses \emph{only} on the observed data, but compares its probabilities of occurrence conditional on different hypotheses. The standard Bayesian point of view agrees that the probability of two hits out of two attempts is $0.4$, but it disagrees with the notion that the epistemically relevant contrast is the one between different counterfactual probabilities based on one hypothesis of interest. Instead, we should be considering which \emph{hypothesis} would have best predicted what we ended up observing. In the context of table \ref{tab:likelihoodvscf}, the Bayesian way is to look \emph{only} at the column for $HH$, the actually observed result, while ignoring all the counterfactual ones. This is summarized as: 

\begin{quote}
THE LIKELIHOOD PRINCIPLE[LP]. All the information about $\theta$ [the parameter of interest] obtainable from an experiemnt is contained in the likelihood function of $\theta$ given $X$. Two likelihood functions for $\theta$ (from the same or different experiments) contains the same information about $\theta$ if they are proportional to another.\footnote{\cite{lp}, 19.}	
\end{quote}


Savage's expression of the principle is perhaps more ambitious:

\begin{quote}
The likelihood principle says this: the likelihood function... is much more than merely a sufficient statistic, for given the likelihood function in which an experiment has resulted, \emph{everything} else about the experiment---what its plan was, what different data might have resulted from it, the conditional distributions of statistics under given parameter values, and so on---is irrelevant. \footnote{\cite{savagerecon}, 583.}	
\end{quote}


We must be clear about the use of the term `likelihood', since it is technical: it refers to the likelihood function \(p(x_{1:n}|\theta)\), which reads: the probability of
observations \(X_1...X_n\) conditional on the parameter $\theta$. $\theta$ in our case is the probability of guessing a card correctly, and, in table \ref{tab:likelihoodvscf}, we are arbitrarily considering a limited set of possible $\theta$s, i.e., $\theta = 0.2, \theta = 0.5, \theta = 1.0$. LP holds that the only epistemically relevant likelihood function is the one represented by the factual column. Here, the result of two hits (HH) is held fixed, while the hypothesized values are varied. In the frequentist method, the hypothesis is held fixed, and the result is varied.


More important, many Bayesians believe that the sort of problem caused by
optional stopping can be explained by its violation of LP, because what counts as the set of all possible scenarios---the number of cells in a row---is entirely dependent on the experiment's intention to stop. But it seems peculiar, the argument goes, that the statistical import of data must be dependent on something that goes on the experiment's mind. For instance, take I. J. Good's assessment of the problem:

\begin{quote}
Given the likelihood, the inferences that can be drawn from the
observations would, for example, be unaffected if the statistician
arbitrarily and falsely claimed that he had a train to catch, although
he really had decided to stop sampling because his favorite hypothesis
was ahead of the game. (This might cause you to distrust the
statistician, but if you believe his observations, this distrust would
be immaterial.) On the other hand, the ``Fisherian'' tail-area method
for significance testing violates the likelihood principle because the
statistician who is prepared to pretend he has a train to catch
(optional stopping of sampling) can reach arbitrarily high significance
levels, given enough time, even when the null hypothesis is true.\footnote{\cite{goodthinking},135}
\end{quote}

% For instance, we needed four cells to represent the space of possibility in table \ref{tab:likelihoodvscf}, because we stipulated that the subject would make two guesses. 

%The crucial point
%here is that the likelihood function holds the actual observations to be
%fixed, while the hypothesized parameter is variable. This is different
%than the frequentist way, in which the \emph{hypothesis} is fixed, and
%asks what the probabilities of different possible outcomes are, if the
%hypothesis were true. This is why Bayesians repeatedly chide
%frequentists for caring about data that we could have but didn't.
%Likelihood function, the Bayesian way of summarizing data, does not take
%into consideration of counterfactual probabilities at all.




At this point, the debate becomes quite messy, since Bayesians tend to
run the problem of optional stopping and the likelihood principle
together, as Good has clearly done in the passage above. The assumption
is that optional stopping cannot occur once Bayesian methods are
adopted. However, the problem of optional stopping is perfectly
intelligible on frequentist ground: it draws out undesirable
consequences based on assumptions accepted by frequentists. The
introduction of LP, however, begs the question against the frequentists.
If all the fundamental frequentist methods violate LP, why would any
frequentist accept this principle? My suspicion is that they probably
won't, so the introduction of LP muddles the water. 

However, what could be motivating Bayesians to see LP as being indispensable
is that it justifies statistical inferences to be made without taking into consideration the intentions of the experimenters. This version of the problem is
summarized as \emph{the argument from intention}, which holds that what
optional stopping shows is that Frequentist's reliance on counterfactual
probabilities renders their result vulnerable to manipulation, because
the experimenter's intention \emph{alone} can radically alter the import
of the evidence.

\hypertarget{intentions-and-self-sabotage-redux}{%
\section{Intentions and Self-Sabotage
Redux}\label{sec:intentions-and-self-sabotage-redux}}
\subsection{Lindley's Argument from Intention}
Consider a simple illustration concerning the bias of a coin discussed
by Lindley and Phillips.\footnote{\cite{lindleybern}, 113--14.} Suppose I was told
that the coin was tossed 12 times but out of those times 3 turned up
heads. The argument from intention says that, unless you know what goes
on inside the tosser mind when she decided to stop the tossing, there is
no way to know what the evidence says. And, depending on the answer she
gives, the evidential import of the result can alter drastically. For
instance, consider these stopping rules:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Stop after 12 tosses
\item
  After 3 heads.
\end{enumerate}

Depending on which of the above rules the
parapsychologist used, the significance of the data will change, even if
the number of guesses and hits are the same. To begin, note that each
rule implies different impossibilities. For instance, if the
experimenter stops after 12 tosses, it is obviously impossible for the
test to last for more than 12 tosses, but it is possible to get anything from 0 to 12 heads. On the other hand, if
the test terminates after 3 heads, the only possible number of heads is
3, but the experiment can take as many tosses as needed to reach that
goal. So each different stopping rule implies different counterfactuals,
and leads to different sets of probabilities.

So Bayesians have an important point here: intentions are influencing
the statistical result through counterfactual probabilities, but, unless
there are reasons to think accounting for intentions is somehow
inherently bad, this \emph{supports} the deliberativist thesis: it argues that deliberating on when to stop based on what would happen after a possible course of action is one of the judgments that shapes the epistemic context, in which the evidential import of the data should be interpreted. Consequently, I am not so much concerned with \emph{solving} the problem by arguing that intentions do matter, but rather with \emph{dissolving} it by making sense of its role in induction. 

To begin, let us spell out what the argument from intention is. Consider the rule that says to stop after \(n=12\) tosses. Using the frequentist method means that we have to consider the probability of all
possible outcomes---0-12 heads, in various combinations.  We know that, from probability theory, for a random
variable with binary outcomes---success or failure, for instance---the
probability of getting \(k\) success out of \(n\) trials, given the
probability of a single success, is

\[{n \choose x} p^x (1-p)^{n-x}\]

Let's say a ``success'' is a coin toss that lands on heads. To carry out
an investigation, we have to make two decisions: the first is to choose
a hypothesis about \(p\) to be tested. In a binomial process with
\(n=12\), assuming that the coin's probability of landing on heads is
\(0.5\), what is the probability that she gets 3 or less heads? That is,
let \(Y=\sum_i^{12} X_i\), where \(X_i = 1\) if the coin lands on heads,
and 0 otherwise, then

\[P(Y \leq 3 ) =  \sum_{i=0}^{3} {12 \choose i} (0.5)^i (0.5)^{12-i} = 0.07\]

This is generally considered an insignificant result, but what if the
intention was to stop whenever the subject has gotten 3 heads? To model
this, we would have to use the so-called negative binomial distribution,
which models the probability of making \(r\) failures before getting \(k\)
successes. In this case, the experimental question is in fact quite
different, since now we would consider the coin to be biased against
heads if it takes an abnormally large number of tosses. So the
statistical question is: what is the probability of the coin needing
\(12\) or more tosses in order to get 3 heads? Using a variant of the binomial distribution called the \emph{negative binomial distribution}, we can
easily find this. Let \(X\) be the number of misses, so

\[P(X \geq 9) = 1 - P(X <9) = 1 - \sum_{i=0}^{8} {3+i-1 \choose i} (0.5)^3 (0.5)^{i} =0.03\]

This seems to be a much more significant result. It seems like we were able to raise the significance of the data by changing nothing but the stopping rule, but this is not the complete picture. 

\subsection{The Deliberativist Analysis}

From the deliberativist perspective, the effect of changing stopping rules is a demonstration of the two key ideas of my position: the Peircean idea that decisions made in abduction contextualize inferences made in induction and the voluntarist ideal of rationality as self-controlled deliberate conduct. According to view I defend, the inferential force of induction is at least partly underwritten by the decisions made during the abductive context. The intention to stop is one such decision. Since the commitment to uphold those decisions is required for the inductive inferences to be considered as valid, changing it afterward is an act of self-sabotage: it defeats the very purpose of trying to probe the hypothesis accepted provisionally. 


As discussion in \ref{sec:the-abductive-context-of-inquiry}, the abductive context is characterized by the freedom of thought to propose and accept a hypothesis provisionally. There is no context-independent justification for the
choice of \(n\), which determines the probability of having \(k\)
successes, but whatever decision one makes has a direct repercussion on how the inference will be made in the inductive context.  For instance, suppose I choose  \(n=5\), and that I decide on the hypothesis that \(p=0.5\). There decisions will put a obligatory constraint on my behavior during the inductive stage, such as when to stop the experiment, and how to revise my belief in light of the evidence. We can, however, explicate these  obligations that these potential decisions imply by engaging in a deductive interrogation, since many of these obligations follow as a necessary consequence of the potential model. For instance, if I choose \(p=0.5\) and \(n=5\), I can deduce that

\[P(X=0) = {5 \choose 0} 0.5^0 (1-0.5)^{5-0} = 0.031\]

\[P(X\leq 1) = \sum_{i=0}^1 {5 \choose i } 0.5^i (1-0.5)^{5-i} = 0.19\]

These are the probabilities that follow deductively from the decisions I
have made in the abductive context. I made what Peirce would call a
\emph{probable deduction}, which involves the deductive derivation of
probabilistic judgments based a model with known parameters. Even though
the conclusion of probable deduction is probabilistic, the
\emph{connection} between the conclusion and its premises is
necessary.\footnote{\cite{probableinference}, 417.}
They signify the epistemic obligation I incur: if I accept
provisionally the hypothesis that \(p=0.5\) for \(n=5\), then I am
committed to the probabilistic judgment that the probability of tossing
the coin five times without heads is 0.031.\footnote{Of course, I could
  be dissatisfied by the result of the deduction, in which case I could
  revise my experimental commitment abductively. I put this issue aside
  now, as the dynamic between abduction and deduction is the focus on
  chapter 4.}

%In any case, Peirce was keenly aware of how intentions could impact the legitimacy of inductive inference. This is why he maintains that the rule of predesignation, says that once I have
%made those decisions, \emph{I cannot revise my },  I cannot, for instance, stop the experiment after
%getting two tails in a row, only because I am interested in proving that
%the coin is unfair. During abduction, I have made the commitment to stop
%the experiment after 5 tosses---changing this intention changes the
%whole inferential context altogether.


% stopping early is an act of
%self-sabotage. This can be shown by pointing out that the probability
%getting two tails out of two throws is \(0.5^2 = 0.25\), which is much
%higher than \(0.031\). 

As Peirce points out, inference made in the inductive context presupposes the experimenter to hold certain values, including

\begin{quote}
first, a sense that we do not know something, second, a desire to know
it, and third, an effort,---implying a willingness to labor,---for the
sake of seeing how the truth may really be.\footnote{\cite{essentialpeirce2}, 48.}
\end{quote}
We can understand the problem of optional stopping as a special
case of \emph{self-sabotage}. The very point of experimentally testing a hypothesis is to see how a it can withstand the deliverance of experience, so knowingly running a test that would likely to give a wrong answer is preemptively sabotaging one's possibility of ``seeing how the truth may really be''. As an analogy, consider making a promise, a speech act we discussed in section. \ref{sec:moores-paradox-and-self-sabotaging} The normative point of making a promise is to express to the promisee your intention to undertake a certain obligation. \footnote{\cite{searle}, 58-59} Because of this, I would be sabotaging myself if I make a promise that is clearly disingenuous. Making a promise I clearly cannot keep is clearly one example, but a more apt analogy would be a promise made in bad faith. For example, consider van Fraassen's example of making a promise with a vacuous conditional statement:\footnote{\cite{empiricalstance}, 243}

\begin{equation}
\text{I promise that if I see Santa Claus, I will ask him to bring you a bicycle.}	
\end{equation}

Such a promise is made in bad faith, because the promiser can never break them. It defeats the purpose of the promise to express an intention, since it's phrased in such a way that the promise is unlikely to be perceived as being sincere. Of course, there is nothing self-sabotaging about this example in and of themselves, but one would be if they are genuinely trying to make a promise. It would be like a person saying to reassure her partner by saying that ``I promise I will \emph{try} to be faithful''. Her partner will hardly be reassured. Analogously, the goal of experimentally probing a hypothesis is to produce evidence that will be accepted by one's epistemic community. The possibility of this acceptance, however, depends on whether or not the agent has genuinely given the hypothesis a chance to face the tribunal of experience, and this is sabotaged by optionally stopping. 

%
%A crucial element in James' voluntarism is that the responsibility
%implied by the inquirer's acceptance of a hypothesis includes the
%incurring the risk of errors in rejecting the alternative hypotheses. To
%get a grip on this risk, the agent must ask: what would happen, had I
%accepted the wrong hypothesis? This style of thinking is already present
%in Pascal's wager, in which he asks us to imagine the scenarios such as
%mistakenly rejecting the existence of God or mistakenly accepting it.

This idea can be accounted for by appealing to the frequentist idea of \emph{error probabilities}. For instance,
suppose I follow the standard practice and declare that I will reject
the hypothesis that \(p=0.5\) when the sample I collect has a less than
the probability of \(0.05\) of occurrence. This would mean that I am
committed in rejecting my hypothesis if I get no heads after tossing
the coin 5 times. But what if, while the coin was actually biased, 
 it was not \emph{so} biased that it will not land on heads at all? The probability of
making such an error can be calculated \emph{ex ante} deductively. For
instance, suppose the reality is that the coin is biased such that
\(p=0.2\). But if this were true, I have a high probability of keeping
my provisionally accepted hypothesis by mistake, because

\[P(X > 0) = 1 - P(X=0) = 1 - {5 \choose 0} 0.8^5 = 0.67\]

That is, it is improbable even for a biased coin to have 5 out of 5 heads. This means that by accepting \(p=0.5\), I am incurring a pretty high
risk of error: if \(p=0.2\), there is a 0.67 probability that my decision would be a wrong one. In fact, the closer \(p\) is to \(0.5\), the more likely it is
that I will come to accept \(p=0.5\) erroneously. If \(p=0.3\), for
instance, this error probability is \(0.83\). \footnote{Of course, the
risk of error is a concern only if I care about avoiding errors and finding the truth. If I care more about just proving my favorite hypothesis, I might just welcome the fact that I get to keep my hypothesis if it was the wrong one. At that point, however, the agent is hardly operating in the context of induction. }


Thus, Lindley is
entirely correct in pointing out that under different stopping rules
would have an impact on what will counted as statistical significance,
even if the numerical result will be the same, but from the
deliberativist standpoint, these stopping rules imply different sets of
epistemic commitments, that is, the experimenter incurs different degrees of the risk of being wrong as indicated by the relevant error probabilities, depending on the decision rules she accepts in the context of abduction.

If the agent intends to stop after 12 trials, then to aim for a level of
statistical significance \(\alpha\) at \(0.05\), she would have to
commit to reject the fair coin hypothesis if she gets 2 or less heads.
The repercussion is that, had the coin been only slightly biased against
landing on heads, it is unlikely that she would be able to find the
truth. For instance, the probability of getting 2 or less heads, if
\(p=0.4\), is only \(0.08\). For \(p=0.3\), it is \(0.25\). In other
words, if the coin were only slightly biased, it would be unlikely to
produce a result that is detectable within this particular
deliberative framework. Of course, there is nothing sacred about
\(\alpha= 0.05\), notwithstanding the preaching of introductory
statistics textbooks. If the agent's intention is to determine if the
coin is only slightly biased, she is free to adjust \(\alpha\) so that
her risk of error, had \(p\) been \(0.4\), is smaller. Even though, if
\(p=0.5\), the probability of getting 3 or fewer heads out of 12 at
\(0.07\) does not quite reach the textbook standard of statistical
significance, it nevertheless would be much better at detecting
\(p=0.4\), since the probability of the same outcome occurring would be
\(0.23\). Not perfect, but this is the kind of decision one makes during
the abductive context.

None of the above considerations hold if we had changed the stopping
rule to ``stop after 3 heads.'' The deliberative framework would be
entirely different. To begin, we are now adjudicating not the
probabilities of error between different numbers of heads landed, but
the number of tails we would tolerate before 3 heads is seen. We saw
that having to see 9 tails before 3 heads is a statistically significant
enough reason to reject the hypothesis that the coin is fair. What this
overlooks, however, is that a biased coin can often get 3 heads before 9
tails; because, probabilities from a negative binomial distribution tend
to be ``front-loaded''. For instance, with a coin that is half as
unlikely to land on heads than tails, that is, \(p=0.25\), the
probability to see 8 or less tails before 3 heads is

\[P(X \leq 8) =  \sum_{i=0}^{8} {3+i-1 \choose i} (0.25)^3 (0.75)^{i} =0.55\]

So looking strictly at the different p-values is a somewhat misleading
way to look at the matter. Mayo and Spanos summarizes the frequentist
response as follows:

\begin{quote}
{[}the argument from intention{]} would seem to beg the question against
the error statistical {[}i.e., frequentist{]} methodology which has
perfectly objective ways to pick up on the effect of stopping rules: far
from intentions ``locked up in the scientist's head'' (as critics
allege), the manner of generating the data alter error
probabilities\ldots{}\footnote{\cite{errorstat}, 186.}
\end{quote}

Of course, a defense of intentions in frequentism is not an argument
\emph{for} the relevance of intention in Bayesianism. Furthermore,
Bayesians like Lindley without a doubt were aware of these basic
statistical facts from power analysis. What prompted their stance is the
assumption that Bayesian methods are impervious to the problem of
optional stopping, since the likelihood function is not affected by
intentions, nor other facts in the deliberative framework. In the next
section, I will demonstrate that optional stopping can also affect
results obtained using Bayesian methods, so I am attacking the very idea
that Bayesians cannot ignore the effects of deliberation.





\hypertarget{bayesian-early-stopping}{%
\section{Bayesian Optional
Stopping}\label{ch: bayesian-early-stopping}}

Deborah Mayo suggests that using early stopping to manipulate experimental results is also possible using Bayesian methods, despite  the Likelihood Principle.\footnote{\cite{errorgrowth},352-353} Mayo's argument appeals to the fact that Bayesian methods can obtain the same or similar result when using a flat/uninformed prior, so this opens the door to early stopping using the same method. I will try to get more mileage out of this argument by using simulations.

To begin, we must gain a basic understanding of what Bayesian inference is, which, naturally, begins with Bayes' theorem. Consider some hypothesis
or belief \(H\) and some evidence \(E\).

\[P(H|E) = \frac{P(H)P(E|H)}{P(E)}\]

In its most basic form, Bayes' theorem has three components: The
unconditional probability of \(H\), \(P(H)\), represents the probability
we would initially assign to the belief. Second, the evidence, as we have
discussed, is represented by the likelihood \(P(E|H)\)---the probability
of the evidence, given the hypothesis is true. The third ingredient is
\(P(E)\), the unconditional probability of \(E\). To see how this works,
consider an example with Zener cards. To begin, suppose that we have a
subject in front of us, and we have to determine if she has ESP. Let's say
she has 2 out of 2 correct answers. How and what should we learn from this data?
What follows is the standard Bayesian story.

For the sake of simplicity, let us suppose for now that there are only
two options: either the subject is randomly guessing, or she has ESP,
which entails a perfect reliability. In other words, we have two
hypotheses. Let \(\theta\) be the subject of probability of getting a
hit, and

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(H_0: \theta = 0.2\)
\item
  \(H_1: \theta = 1\)
\end{enumerate}

These are sometimes called ``chance hypotheses''. Now let \(E_i\) refer
to the result of the \(i\)th guess, such that $E_i=1$ for a hit, and
\(0\) otherwise. So let \(E = \sum_i^2 E_i = E_1 + E_2 = 2\). This means
that we have the following likelihoods:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  \(P(E|H_0) = 0.2^2=0.04\)
\item
  \(P(E|H_1) = 1\)
\end{enumerate}

Recall that the likelihood principle says that this contains all the
information we need to know about the experiment. Now, suppose you are
not a believer of ESP, so you are almost certain---say, \(99\%\)
sure---that the subject will not do better than chance. We then have the
priors needed:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  \(P(H_0) = 0.99\)
\item
  \(P(H_1) = 0.01\)
\end{enumerate}

From the above, we can derive
\[P(E_i = 1) = P(H_0)P(E_i=1|H_0) + P(H_1)P(E_i=1|H_1) = 0.99(0.04)+0.01(1) = 0.0496\]

Using Bayes' theorem, we can then revise our belief about the subject's
ability to guess cards, producing the following \emph{posterior
probabilities}:

\[P(H_0|E) = \frac{0.99(0.04)}{0.0496} = 0.8\]
\[P(H_1|E) = \frac{0.01(1)}{0.0496} = 0.2\]

Having seen two successful attempts in a row, we have warmed up to the
idea that the subject might have ESP. An intuitive way to look at this
Bayesian procedure is that the posterior probability is a promise
between my existing belief---my priors---and evidence, which is
summarized by the likelihoods, according to the LP.

To make my point, these basic Bayesian statistical procedures are
sufficient, though things will get somewhat messy when we consider more
realistic cases. For instance, it is arbitrary to consider only two
chance hypotheses. A more applicable model would be to consider all
possible values of \(\theta\) in \([0,1]\). For this we have to use some
of the well-established distributions. I will again use simulation
to demonstrate the effect of optional stopping, but to do so I need to first explain
how the situation will be modeled.

Recall that optional stopping from a frequentist context entails
falsely rejecting the null hypothesis by sampling over and over again until
obtaining an outcome with a probability low enough on the null
hypothesis to secure statistical significance. The Bayesian parallel is
to continue sampling so we can have \(E\) such that \(P(H_0|E) < x\)
where \(x\) is a value that the optional stopper is committed to believing.
Note that now we concern ourselves with the probability of the hypothesis
itself, whereas in the frequentist setting we were concerned with the
probability of the observation.

Fortunately, since there are only two outcomes, a Zener card-type
experiment can be modeled as a Beta-Bernoulli process, where the Beta
distribution would model our degrees of belief about a subject's
\emph{propensity} and the Bernoulli distribution would represent the
Zener card experiment itself. What these models represent is usually
clear enough in a practical and statistical setting, but since we are doing philosophy, we need to be clearer about what we mean by
degrees of belief and propensity, so we are clear about the phenomena being
modeled.

I suggest we follow the views of D. V. Lindley and David Lewis. Lindley
argues that probability is a relation between the agent and the world,
so when we say \(P(\theta=0.5)\), it represents our epistemic judgment about some physical event \(\theta\).\footnote{\cite{lindleybern}, 115.} In our case, this
has to be an objective reliability of the subject's ability to discern the face of the card, which
is a property in the world: even though \(\theta\) looks like a
probability, in the Bayesian statistical framework we can just treat it
as another parameter being modeled, not unlike \(\mu\) or \(\sigma\) for
normal distributions, so a subject's extra-perceptual reliability is an
objective feature of the world in a way no different than the fact that
the average age of Duke students is an objective fact. The degree of
beliefs about them, according to the voluntarist conception I accept, is an epistemic judgment made about this objective fact.
 
 This recommendation is compatible with, if not the same as, the influential
view presented by David Lewis, who adopts Carnap's pluralistic stance on
probability. Carnap thinks there are at least two concepts of
probability: \(probability_1\), which is an epistemic concept about
degrees of confirmation and \(probability_2\), which refers the
empirical concept of long-run limiting frequencies.\footnote{\cite{carnapprob}, 517.} Lewis suggests
that we should instead interpret the epistemic concept as credence or
degree of belief and the empirical concept as chance or
propensity.\footnote{\cite{lewisguide}} So, following Lewis, we can interpret \(P(\theta=0.5)=x\) to
be ``the degree for the belief that the chance of heads is 0.5 is
\(x\).'' For the sake of consistency, I will refer to subjective
probability just as \emph{credence} or \emph{degrees of belief}, and
objective probability as \emph{chance} or \emph{propensity}.

We can now spell out the type of experiments to be simulated.

Early on, we considered a case in which only two possible hypotheses are
considered: either the subject is guessing
randomly(\(H_0:\theta = 0.2\)) or the subject has perfect
reliability(\(H_1:\theta = 1\)). This makes our epistemic attitude
relatively easy to summarize, since all we have to do is to assign a
value to our credence to each of the two hypotheses. As we noted, this
is an oversimplification, since there is no reason to arbitrarily
restrict ourselves to just two hypotheses. This, however, means that we
need a way to deal with the fact that there are infinitely many possible
hypotheses between 0 and 1, which is why we need the beta distribution.

The beta distribution is really nothing but a function that, based on
two parameters we provide, describes our epistemic attitudes toward
\(\theta\).\footnote{The distribution has the form:
  \(\frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}x^{\alpha -1} (1-x)^{\beta -1}\)
  where the parameters \(\alpha,\beta > 0\) and
  \(0\leq x \leq 1\)(\(x\) is the random variable being
  modeled) } The two parameters, \(\alpha>0\) and \(\beta>0\), can be
thought of as, in our context, our past experience about \(\theta\),
with \(\alpha\) representing past successes and \(\beta\) past failures.
For instance, if we set \(\alpha = \beta = 1\), it should say that we
are extremely ambivalent about \(\theta\). In fact, it is equivalent to
having a uniform distribution over \([0,1]\)---this means that I am
utterly indifference regarding any value for \(\theta\).

Our data-collection will be modeled using the binomial distribution. Let
\(x\) be the number of success, \(n\) the number of trials, and
\(\theta\) the propensity of success:

\[f(x|\theta, n) = {n\choose k} \theta^k (1-\theta)^{(n-k)}\]

This is the same distribution we used as the sampling distribution in
the frequentist case, but recall that for Bayesian analysis we will no
longer concern ourselves with counterfactual probabilities, instead, we
are treating \(\theta\) as the function of the \(x\), the number of
success which is constant.

Fortunately, as soon as this is laid out, the rest is very simple,
thanks to the fact that the beta distribution is a \emph{conjugate
prior} for the binomial distribution.\footnote{\cite{degroot}, section 7.3} Essentially, what this mean is
just that if we plug the beta and binomial distributions into Bayes'
theorem to get a posterior distribution

\[ p(\theta|x) = \frac{p(\theta)p(x|\theta)}{\int p(\theta)p(x|\theta)}\]

\noindent the result is simply another beta distribution with parameters
\(\alpha =\alpha + x\) and \(\beta = \beta + n - x\). In words, to learn
from experience, all we have to do is to add the number of successes to
\(\alpha\) and the number of failures to \(\beta\). Another useful thing
to keep in mind is that the beta distribution's expected value has the
form:

\[E(\theta) = \frac{\alpha}{\alpha + \beta}\]

\noindent Now, thanks to conjugacy, the \emph{posterior} expected value
is simply:

\[E(\theta) = \frac{\alpha + k}{\alpha + \beta + n}\]

So, our experimental procedure is fairly simple: we pick an appropriate
set of parameters, and for each trial in which the subject is able to
guess the card correctly, we add 1 to \(\alpha\); otherwise, we add 1 to
\(\beta\). For example, consider again the case of a skeptic who
observed that a subject has made two correct guesses in a row. Since
prior to the observation the skeptic does not believe that the subject
would do better than chance, she knows that

\[E(\theta) = \frac{\alpha}{\alpha + \beta} = \frac{1}{5}\]

There are various ways in which we can make this work mathematically,
but for now let's say \(\alpha = 2\) and \(\beta = 8\).\footnote{The epistemological relevance of the choice of these parameters will the discussed in chapter \ref{ch:woe}.} Since the
subject has gotten 2 out of 2 correctly, the skeptic's posterior should
be the beta distribution with \(\alpha= 2+2=4\), while \(\beta\) remains
at 8.

Because we are using Bayesian methods, we can ask directly the
probability of \(\theta\) having certain values. A similar question we
can ask, then, \emph{given} the evidence we have, what is the
probability of the subject's para-perceptual reliability is no better
than randomly guessing? In other words, what is the probability that
\(\theta\) is less than or equal to \(0.2\)? Using the cumulative
distribution function for the beta distribution provided by computer programs, we can find out the prior and posterior values:

\[P(\theta \leq 0.2) = 0.56\] \[P(\theta \leq 0.2|\mathbf{X}) = 0.16\]

We can see that after witnessing the evidence, the skeptic's personal
probability for the belief that the subject is doing no better than
chance is lowered by quite a bit. The Bayesian version of optional
stopping is this: a Bayesian optional stopper can decide to stop
gathering more evidence as soon as the posterior is low enough. The idea
is that a committed enough optional stopper will eventually find
``evidence'' for ESP, i.e., subjects with low posterior probability of
random guessing.

To simulate this procedure, we will carry out the following:



\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  For each simulation in \(n\) times:

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    For each subject \(s\), we begin with a flat uniform distribution by
    setting \(\alpha = \beta = 1\).
  \item
    A random sample \(x_i\) will be drawn from a Bernoulli distribution,
    with \(\theta = 0.2\).
  \item
    Add 1 to \(\alpha\) if \(x_i =1\); add 1 to \(\beta\) otherwise.
  \item
    Terminate if either (i) \(P(\theta_s \leq 0.2|\mathbf{X})\) is less
    than threshold \(k\leq 0.05\) or (ii) the number of trial \(i\) has exceeded
    the maximum number \(m=2000\). Otherwise, return to step a with the same
    subject.
  \end{enumerate}
\item
  If \(n\) subjects have been tested, terminate; else, return to step a
  with a new subject.

\end{enumerate}



\begin{figure*}[h] 
\begin{center}
\includegraphics[scale=0.5]{Bayesstopflatallpost.png}
	\caption{Histogram of $p_i(\theta\leq 0.2|x,\alpha=1, \beta=1)$}
	\label{fig:bayesflatposterior}
\end{center}	
\end{figure*}

\begin{figure*}[h] 
\begin{center}
\includegraphics[scale=0.5]{bayesstop14allpost.png}
	\caption{Histogram of random samples from $p_i(\theta\leq 0.2|x,\alpha=1, \beta=4)$}
	\label{fig:bayes14posterior}
\end{center}	
\end{figure*}

$n=1000$ stimulations were carried out. We clearly see the effect of early stopping here. In fact, using a flat prior, $\alpha=\beta=1$, 517 out of 1000 stimulations were stopped early due to reaching a low enough posterior probability for $p(\theta \leq 0.2)$, which more than the number we had for frequentist early stopping. The distribution of the posterior probabilities can be seen in figure \ref{fig:bayesflatposterior}. This is partly due to the fact that, when starting from a flat prior, the  experiment can stop after 1 guess if the first guess is the correct one, because with $\alpha = 2, \beta=1$, the posterior probability for $P(\theta\leq 0.2 = 0.04$, which is already lower than the threshold designated. 208 out of 1000 simulations ended this way, which is exactly what was expected, since we are drawing from a Bernoulli distribution with the probability $0.2$ of success. 

To eliminate that particular result, we can use a weakly informed prior, by setting $\alpha = 1, beta=4$. The result is summarized in figure \ref{fig:bayes14posterior}. Since the initial expected value is $0.2$, and this means that for the cases in which early stopping is successful, we have essentially manipulated ourselves from the right opinion into the wrong one. The result is nearly identically to the frequentist case, with 367 out of 1000 results being stopped early to get the posterior probability desired.


Based on the above, one may argue that optional stopping could further prevented by using a even strongly informed prior. This is true: for instance, if, instead of
having \(\alpha = \beta = 1\) as parameters, we use something strongly
biased in favor of \(\theta = 0.2\), such as
\(\alpha = 10, \beta = 40\), it would be fairly difficult for the
optional stopper to game the result. But this makes sense only because we \emph{know} what the true distribution is. This also seems to me a point
\emph{for} deliberativism, not against it, because this is amount to
saying commitments and intentions matter by sneaking them in through the
back door of priors. Furthermore, what is stopping an optional stopper
to cheat even more by adopting a set of parameters that is biased
\emph{against} \(\theta = 0.2\)? Of course, we would criticize anyone
that adopts such an experimental stance, but it would be made on the
deliberativist ground: the standard of criticisms do consider intentions and decisions as relevant information.

\hypertarget{is-optional-stopping-irrational-from-the-perspective-of-the-utility-maximizer}{%
\section{Is Optional Stopping Irrational From the Perspective of the
Utility
Maximizer?}\label{is-optional-stopping-irrational-from-the-perspective-of-the-utility-maximizer}}

There is, however, a more substantive objection that requires a more
thorough exposition. An Orthodox Bayesian could argue that optional
stopping is an irrational practice, because from a Bayesian perspective
it is never rational to refuse evidence, because additional evidence
\emph{always leads the increase in expected utility}. This is in fact a
result that has been proven in various occasions and forms, first by Frank
Ramsey, and then later, independently, I. J. Good, and L. J. Savage.\footnote{\cite{ramseyvalue}}\footnote{\cite{goodtotalevidence}}\footnote{\cite{savage}, sec 6.2.} Skyrms has also discussed Ramsey's unpublished note in details.\footnote{\cite{rationaldel}, chapter 4.}

Some context is helpful. In his \emph{A Treatise in Probability}, J. M.
Keynes points out that subjectivists and expected utility theorists
often implicitly assume that we should always get more evidence.
Bernoulli, for instance, suggests that rationality demands the
utilization of all evidence available to us. This implies, Keynes
thinks, that it's always rational to get more evidence, but then it
raises another critical question about whether or not one could ever be
rational in refusing new evidence.\footnote{\cite{keynes}, 84--85.} If the answer for the former question is
positive, and the latter question negative, then we have to conclude
that rationality dictates us that we should never stop looking for more
evidence. Keynes remains ambivalent about the notion itself. Ramsey, who clearly read \emph{a Treatise}, responded to the problem in an
unpublished note.

Many years later Ayer raises the same question in
response to Carnap's \emph{Logical Foundation of Probability}, in which
Carnap essentially restates Bernoulli's maxim as ``the requirement of
total evidence''.

\begin{quote}
\emph{Requirement of total evidence}: in the application of inductive
logic to a given knowledge situation, the total evidence available must
be taken as basis for determining the degree of confirmation.\footnote{\cite{carnapprob}, 211.}
\end{quote}

Ayer asks the Keynesian question: should ``total evidence'' include
relevant evidence that I do not yet have in possession?\footnote{\cite{ayerpae}, 56.} The
answer must be ``yes'', Ayer argues. If finding the truth value of some
proposition \(P\) could potentially sway the balance of my evidence,
then I should definitely acquire it. Thus the principle of total
evidence seems to suggest that I am also rationally compelled to
consider some evidence I do not yet have. I. J. Good interprets Ayer's as questioning ``why\ldots{} we should
bother to make new observations.''\footnote{\cite{goodthinking}, 178.} In the context of optional stopping, this
is particularly salient: if I already have the result I want, why should
I bother get more evidence?

Ramsey interpreted Keynes' question in the same way and addressed it an unpublished note addresses  from an expected utility perspective. Ramsey's argument is roughly that,
\emph{if} we assume an agent to be a perfect Bayesian and that new
information does not cost anything, then she will never be no worse off
getting new evidence. In fact, she is guaranteed to be \emph{better} off
as long as the new evidence will tell her something new. A perfect
Bayesian agent is someone who studiously updates her opinions based on
Bayes' rule and then act by choosing the action that maximize her
expected utility. Note that this assumes a few things: first, for any
decision problem she faces, there is always going to be at least one
option that maximizes her expected utility. Second, this assumes that she can always assign precise probabilities to the options she faces, because, as Good later realizes, if the agent's judgments of probability is vague, that is, she may judge the probability of an event to fall within a range of values, instead of one precise numerical value, then it is possible that confounding observations could muddle the water by widening this range, so that she could be worse off by getting more evidence.\footnote{\cite{goodthinking}, 181-182} Third, as Skyrms points
out, this also implies that the agent knows that she will always
\emph{stays} being perfectly Bayesian in the future. What we have here,
then, is the ideal Bayesian agent.

I will make use of an intuitive example rather than reproducing the
proof here.\footnote{This example is adapted from \cite{leviweight}, 49.} Suppose we have
three hypotheses about the content of an urn in front of us:


\forestset{
 declare toks={elo}{}, % Edge Label Options
 anchors/.style={anchor=#1,child anchor=#1,parent anchor=#1},
 dot/.style={tikz+={\fill (.child anchor) circle[radius=#1];}},
 dot/.default=2pt,
 decision edge label/.style n args=3{
 edge label/.expanded={node[midway,auto=#1,anchor=#2,\forestoption{elo}]{\strut$\unexpanded{#3}$}}
 },
 decision/.style={if n=1
 {decision edge label={left}{east}{#1}}
 {decision edge label={right}{west}{#1}}
 },
 decision tree/.style={
 for tree={
 s sep=0.5em,l=8ex,
 if n children=0{anchors=north}{
 if n=1{anchors=south east}{anchors=south west}},
 math content,
 },
 anchors=south, outer sep=2pt,
 dot=3pt,for descendants=dot,
 delay={for descendants={split option={content}{;}{content,decision}}},
 }
}
\begin{figure*}[h]
\centering
\begin{forest} decision tree
 [No Evidence,align=center
 [Decide;EV:1/3,elo={yshift=4pt},align=right 
 [H_b[\bf{1/3}]]
 [H_w[\bf{1/3}]]
 [H_n[\bf{1/3}]]
 ]
 [Observe;EV:0.6,plain content,elo={yshift=4pt}
 [;b(0.5)
 [H_b[\bf{.600}]]
 [H_w[.067]]
 [H_n[.330]]
 ]
 [;w(0.5)
[H_b[.067]]
 [H_w[\bf{.600}]]
 [H_n[.333]]
 ]
 ] 
 ]
\end{forest}
	\caption{Decision tree for no existing evidence(uniform prior).Utility maximizing options are bolded.}
	\label{fig:dectreeuniform}
\end{figure*}


\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(H_b\): 90 black balls and 10 white balls
\item
  \(H_w\): 10 white balls and 90 black balls
\item
  \(H_n\): 50 white balls and 50 black balls.
\end{enumerate}

Suppose we start by assuming \(P(H_b) = P(H_w) = P(H_n) = 1/3\)---we
could have some knowledge that assures us that these are the only three
possibilities. There is also a reward of \$1 for picking the correct
hypothesis. Our expected payoff for choosing each hypothesis would be
the same at \(1/3\). Nevertheless, we are allowed to sample with
replacement as many times as we wish. Should we get more evidence? Yes,
according to Ramsey, we should, and this can be demonstrated in an
expected utility analysis.

This situation is represented as a decision tree in figure \ref{fig:dectreeuniform}: at the beginning, the probability of getting a black ball is the
same as getting a white ball. Let \(E_b\) be ``a black ball is drawn''
and \(E_w\) for white balls. So:
\[P(E_b) = P(H_b)P(E_b|H_b) + P(H_w)P(E_b|H_w) + P(H_n)P(E_b|H_n)\]
\[=1/3(0.9)+1/3(0.1)+1/3(0.5)=0.5\] And \(P(E_w) = 1 - P(E_b) = 0.5\).
So, in the event of drawing a black ball from the urn, we would update
our belief like so:

\[P(H_b|E_b) = \frac{P(H_b)P(E_b|H_b)}{P(E_b)}=\frac{1/3(.9)}{.5} = .600 \]

Similarly, applying the calculation on the other hypotheses, we get:
\[P(H_w|E_b) = .067\] \[P(H_n|E_b) = .333\] Similar argument can be
made by assuming \(E_w\), that is, a white ball is chosen. In that case
\(P(H_w|E_w) = .600\). If we were an ideal Bayesian agent, we should pick
\(H_b\) if \(E_b\), and pick \(H_w\) if \(E_w\). Since an ideal Bayesian
would choose the option that maximizes our expected utility, in either
case the expected value after drawing from the urn once is \(.600\),
which is an improvement, since before drawing our expected utility is
\(1/3\) for all options. The net gain in expected utility would be
\(.60 - .33 = .27\), is referred to as \emph{the value of information}
in the decision theory literature.\footnote{\cite{appliedstatdec},
  89--90}\footnote{\cite{winkler}, section 6.3.}
  

\begin{figure*}[h]
\centering
\begin{forest} decision tree
 [After 1 Observation($b$),align=center
 [Decide;EV:0.6,elo={yshift=4pt},align=right 
 [H_b[\bf{.600}]]
 [H_w[.067]]
 [H_n[.333]]
 ]
 [Observe;EV:0.706,plain content,elo={yshift=4pt}
 [;b(.71)
 [H_b[\bf{.757}]]
 [H_w[.009]]
 [H_n[.233]]
 ]
 [;w(.29)
[H_b[.210]]
 [H_w[.210]]
 [H_n[\bf{.580}]]
 ]
 ] 
 ]
\end{forest}
	\caption{Decision tree after observing one black ball). Utility maximizing options are bolded.}
	\label{fig:dectreepost}
\end{figure*}


It turns out that we would be even better off if we were to draw from
the urn again. This situation is represented as a decision tree in figure \ref{fig:dectreepost}: suppose the first draw yields a black ball. So now we
have one piece of evidence in hand. Let us refer to our state of belief
after the first draw as \(H_b', E_b',..\) and so on. For instance,
\(P(H_b') = P(H_b|E_b)\) and \(P(E_b') = P(E_b'|E_b)\). One notable
change is that \(P(E_b') = .71\) and \(P(E_w')=.29\). If we draw
again and get a black ball, this means:

\[P(H_b'|E_b') = .757\] \[P(H_w'|E_b') = .009\]
\[P(H_n'|E_b') = .233\]

If a white ball were to be drawn:

\[P(H_b'|E_w') = .210\] \[P(H_w'|E_w') = .210\] \[P(H_n'|E_w') = .580\]

Thus, if the second sample is a black ball, we would choose \(b\) since
it has the maximum expected utility at \(.757\), and if we get a white
ball, we choose \(n\) with the expected value at \(.580\). So, the
expected utility, if we were to draw from the urn again, is:
\(.7132(.757) + .2867(.580) = .706\), which is an improvement over
just drawing once. The net gain is \(.706 -.600 = .106\). Ramsey's
proof shows that we can keep on getting more evidence and we will never
be worse off. 

To see a bird's-eye view of the situation, consider figure \ref{fig:expectedchange}, in which the sequential updatings of expectation after drawing from different hypothesized distribution are plotted. Each line represent the \emph{net change} of expected value of getting more evidence conditional having $x$ observation. We can see that the expected net change will never dip below $0$, meaning that one will always often be rewarded and never punished for getting more evidence.

\begin{figure*}[h] 
\begin{center}
\includegraphics[scale=0.5]{Expectedchange.png}
	\caption{Net gain in expectation after $x$ observations from $p=.9,.1,.5$. Note: it does not dip below 0, which implies that one is never worse off from getting new evidence.}
	\label{fig:expectedchange}
\end{center}	
\end{figure*}


This result could be used to answer the Bayesian version of optional
stopping in this way: since getting more evidence always yields better
expected values, the ideal Bayesian agent will always opt for more
evidence, instead of stopping ahead just because the posterior has
reached her favorite degree.

However, I do not think this answer will do. To begin, the crucial
assumption here is that evidence costs \emph{nothing}. The scenario we
imagined quickly breaks down once we starts to introduce some sort of
cost. It was assumed in the example that it costs us neither money nor
time to draw from the urn, but suppose it costs us 25 cents for each
sample. This means that we would be gaining only \(.27-.25 = 0.02\) in
expected payoff for the first draw, and the second draw would definitely
not be worth the additional 25 cents. Or suppose that one dollar is not
worthy any endeavor that lasts longer than 15 seconds, and it takes 30
seconds to draw from the urn. As soon as minimally realistic assumptions
are introduced, Ramsey's result no longer holds.

Cost might also enter into consideration in different forms. Savage
discusses a case in which a very ill person, who is given a cost-free option
to find out if the disease she has is terminal. Savage points out that an argument can be made that in this case refusing information could be rational. The thought is that the patient
may decide that, based on an assessment of her own personality, she
would live the rest of her remaining life in agony if she were to find
out that her disease is very serious, whereas she could live relatively
happily without knowing. Savage's point is that in this case the
information is not really free; it has a \emph{psychological}
cost.\footnote{\cite{savage}, 107.} 

Ramsey and Good's proofs, while extremely valuable from a logical and
mathematical perspective, are somewhat tone-deaf to the practical problem
posed by Ayer and Keynes. The actual complaint was that the Principle of
Total Evidence \emph{presupposes} that we know ahead what ``total
evidence'' amounts to, since the decision to get more evidence or simply
sticking to our current body of evidence is not one that be resolved
just by appealing to probability, because the rationality of such a
decision is highly context-sensitive. One important context is the
\emph{urgency} of decision. In many cases time is of the essence: ``a general who refused to
launch an attack until he had ascertained the position of every enemy
solider would not be very successful.''\footnote{\cite{ayerpae}, 57.}

The economist G. L. S. Shackle makes a similar point engagingly by
retelling the thought process of a certain Chinese guard who had to
decide on the spot whether or not to join the rest of the guards to
partake in a rebellion or to be the lone loyalist to stand in defense of
the empire. He argues that it would be rather foolish to suggest that
the guard should maximize his utility by looking for more evidence:

\begin{quote}
{[}Had the guard taken heed of the advice given by the expected utility
theorist,{]} he might have argued thus: `I find in the record of history
a thousand cases similar to my own, wherein the person concerned decided
upon treachery, and in only four hundred of these case the rebellion
failed to and he was beheaded. On balance, therefore, the advantage
seems to lie with treachery, provided one does it often enough'\ldots{}
Had the sentry decided to support the rebellion, he might have had time,
just before the axe fell, to reflect that he would never, in fact, be
able to repeat his experiment a thousand times, and thus the guidance
given him by actuarial considerations had proven illusory.\footnote{G.L.S.
  Shackles, \emph{Uncertainty in Economics and Other Reflections}
  (Cambridge University Press, 1955), 2.}
\end{quote}

My point, of course, is not that making decision based on probability
and utility is irrational. Far from it, but that rational
inductive thinking presupposes a deliberative framework. The context of
the story makes it clear that for the guard, ``total evidence'' really
just means whatever he has in mind at the moment, and it would be
irrational to suggest that he should get more evidence just because his
expected utility will improve. As Ayer suggests, Carnap's inductive logic presupposes that the agent already know what total evidence is, and the extent to which new evidence is warranted.

Good, who proved the same result independently of Ramsey, tries to
address this issue by distinguishing what he calls Type I and Type II
rationality.\footnote{\cite{goodthinking}, 29-30. As far as I
  could tell, this has nothing to do with the distinction between Type I
  and Type II error in frequentist statistics.} Type I rationality is
that of the ideal Bayesian agent, one who lives her life by abiding to
the principle of maximizing expected utility. Good recognizes, however,
that type I rationality provides no guidance in regard to when an
investigation should be concluded. This is where type II rationality
comes in: it is principle of maximizing expected utility plus the
consideration of ``the cost of theorizing.'' More important, the goal of
type II is ``a sufficient maturity of judgments.''\footnote{\cite{goodthinking}, 29.}

Good's two types of rationality could be interpreted as a concession to
there is a level of rational criticism that cannot be captured within
the strict framework of expected utility. Phrases such as ``cost of
theorizing'' and ``maturity of judgment'', it seems to me, are simply
other way to express the intention to stop. Intentions are, after all,
relevant in Bayesian reasoning.

\section{Conclusion}\label{stoppingcon}

My focus of this section was to elucidate the very idea that the commitment made in the abductive context has  repercussion on the inference made during the inductive context. The problem of optional stopping in parapsychology has served as a helpful case to demonstrate the issues at stake. Even though this problem has been traditionally associated with frequentist methodologies, I have tried to show that this is a problem that should also concern Bayesians. My contention is that aspects of inductive thinking has to be criticized in light of the deliberation the inquirer undergoes prior the experiment, such as the commitments and intentions considered and decided on by the experimenter. I explored a Bayesian response to optional stopping, which relies on a mathematical result, proven by Ramsey and others, that one's expected value never decreases from gathering new evidence, and in many cases it actually increases. I dismissed this line of thought, because the result only holds when the evidence is cost-free, which is almost never the case.

 